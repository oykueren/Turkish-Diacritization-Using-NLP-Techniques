{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from unidecode import unidecode\n",
    "import time;\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, BertModel, AutoConfig, AutoTokenizer\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import zemberek\n",
    "from zemberek.morphology import TurkishMorphology\n",
    "from zemberek.morphology.analysis.rule_based_analyzer import RuleBasedAnalyzer\n",
    "from zemberek.normalization import TurkishSpellChecker\n",
    "from zemberek.morphology.morphotactics import TurkishMorphotactics, InformalTurkishMorphotactics\n",
    "#import RootLexicon\n",
    "from zemberek.morphology.lexicon import RootLexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-crf\n",
    "# !pip install unidecode\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install --upgrade torch\n",
    "# !pip install zemberek-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    \n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_train=pd.read_csv(\"/kaggle/input/nlp-project-train/train.csv\", index_col=[0])\n",
    "# nlp_test=pd.read_csv(\"/kaggle/input/nlp-project-train/test.csv\",index_col=[0],encoding=\"windows-1252\") \n",
    "\n",
    "nlp_train=pd.read_csv(\"train.csv\", index_col=[0])\n",
    "nlp_test=pd.read_csv(\"test.csv\",index_col=[0],encoding=\"windows-1252\") \n",
    "\n",
    "nlp_train = nlp_train.iloc[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_chars_dict = {\n",
    "    \"A\": \"a\",\n",
    "    \"B\": \"b\",\n",
    "    \"C\": \"c\",\n",
    "    \"Ç\": \"ç\",\n",
    "    \"D\": \"d\",\n",
    "    \"E\": \"e\",\n",
    "    \"F\": \"f\",\n",
    "    \"G\": \"g\",\n",
    "    \"Ğ\": \"ğ\",\n",
    "    \"H\": \"h\",\n",
    "    \"I\": \"ı\",\n",
    "    \"İ\": \"i\",\n",
    "    \"J\": \"j\",\n",
    "    \"K\": \"k\",\n",
    "    \"L\": \"l\",\n",
    "    \"M\": \"m\",\n",
    "    \"N\": \"n\",\n",
    "    \"O\": \"o\",\n",
    "    \"Ö\": \"ö\",\n",
    "    \"P\": \"p\",\n",
    "    \"R\": \"r\",\n",
    "    \"S\": \"s\",\n",
    "    \"Ş\": \"ş\",\n",
    "    \"T\": \"t\",\n",
    "    \"U\": \"u\",\n",
    "    \"Ü\": \"ü\",\n",
    "    \"V\": \"v\",\n",
    "    \"Y\": \"y\",\n",
    "    \"Z\": \"z\",\n",
    "    \"Q\": \"q\",\n",
    "    \"W\": \"w\",\n",
    "    \"X\": \"x\"    \n",
    "    }\n",
    "    \n",
    "words_dict = {\n",
    "    \"ı\": \"i\",\n",
    "    \"ğ\": \"g\",\n",
    "    \"ü\": \"u\",\n",
    "    \"ş\": \"s\",\n",
    "    \"ö\": \"o\",\n",
    "    \"ç\": \"c\",\n",
    "    \"İ\": \"I\",\n",
    "    \"Ğ\": \"G\",\n",
    "    \"Ü\": \"U\",\n",
    "    \"Ş\": \"S\",\n",
    "    \"Ö\": \"O\",\n",
    "    \"Ç\": \"C\"\n",
    "    }\n",
    "\n",
    "characters_dictionary = {\n",
    "    \"a\":1,\n",
    "    \"b\":2,\n",
    "    \"c\":3,\n",
    "    \"ç\":4,\n",
    "    \"d\":5,\n",
    "    \"e\":6,\n",
    "    \"f\":7,\n",
    "    \"g\":8,\n",
    "    \"ğ\":9,\n",
    "    \"h\":10,\n",
    "    \"ı\":11,\n",
    "    \"i\":12,\n",
    "    \"j\":13,\n",
    "    \"k\":14,\n",
    "    \"l\":15,\n",
    "    \"m\":16,\n",
    "    \"n\":17,\n",
    "    \"o\":18,\n",
    "    \"ö\":19,\n",
    "    \"p\":20,\n",
    "    \"r\":21,\n",
    "    \"s\":22,\n",
    "    \"ş\":23,\n",
    "    \"t\":24,\n",
    "    \"u\":25,\n",
    "    \"ü\":26,\n",
    "    \"v\":27,\n",
    "    \"y\":28,\n",
    "    \"z\":29,\n",
    "    \"q\": 30,\n",
    "    \"x\":31,\n",
    "    \"w\":32,\n",
    "    \" \":33,\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ascii(sentence):\n",
    "    # convert Turkish characters to English characters\n",
    "    for key, value in words_dict.items():\n",
    "        sentence = sentence.replace(key, value)\n",
    "    return sentence\n",
    "\n",
    "def remove_puncutations_and_numbers(text):\n",
    "    # Iterate over the string and remove char if it is not a character\n",
    "    characters = \"abcçdefgğhıijklmnoöprsştuüvyzABCÇDEFGĞHIİJKLMNOÖPRSŞTUÜVYZxXwWqQ \"\n",
    "    for char in text:\n",
    "        if char not in characters:\n",
    "            text = text.replace(char, \"\")\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    # Lowercase the text\n",
    "    # text = text.lower()\n",
    "    return text\n",
    "\n",
    "def split_sentences(sentences, max_length=100):\n",
    "    punctuations = {'.', ',', ';', ':'}\n",
    "    results = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        while len(sentence) > max_length:\n",
    "            # find last punctuation before max_length\n",
    "            split_pos = -1\n",
    "            for p in punctuations:\n",
    "                pos = sentence.rfind(p, 0, max_length + 1)\n",
    "                if pos > split_pos:\n",
    "                    split_pos = pos\n",
    "            \n",
    "            # If no punctuation found, split at the last space before max_length\n",
    "            if split_pos == -1:\n",
    "                split_pos = sentence.rfind(' ', 0, max_length + 1)\n",
    "            \n",
    "            # If no space found, just split at max_length\n",
    "            if split_pos == -1:\n",
    "                split_pos = max_length\n",
    "            \n",
    "            # Append the split segment to results\n",
    "            results.append(sentence[:split_pos + 1].strip())\n",
    "            # Move the rest of the sentence forward\n",
    "            sentence = sentence[split_pos + 1:].strip()\n",
    "        \n",
    "        # Append the remainder of the sentence if it's not empty\n",
    "        if sentence:\n",
    "            results.append(sentence)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def padding(text, filling_char, max_length):\n",
    "    if type(text) is str:\n",
    "        text = text + filling_char * (max_length - len(text))\n",
    "    elif type(text) is list:\n",
    "        text = text + [filling_char] * (max_length - len(text))\n",
    "    return text\n",
    "\n",
    "def map_diacritics(text):\n",
    "    for char_index in range(len(text)):\n",
    "        # print(text[char_index])\n",
    "        if text[char_index] in \"ıöüğşç\":\n",
    "            text[char_index] = 2\n",
    "        elif text[char_index] in \"iougsc\":\n",
    "            text[char_index] = 1\n",
    "        elif text[char_index] == \" \":\n",
    "            text[char_index] = 0\n",
    "        else:\n",
    "            text[char_index] = 3\n",
    "        \n",
    "    return text\n",
    "\n",
    "def prepare_train_dataset(sentences):\n",
    "    print(sentences)\n",
    "    processed_sentences = []\n",
    "    # Iterate over the sentences\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Remove punctuations and numbers\n",
    "        sentence = remove_puncutations_and_numbers(sentence)\n",
    "        # Split the sentence into smaller sentences\n",
    "        splitted_sentences = split_sentences([sentence])\n",
    "        # print(splitted_sentences)\n",
    "        # break\n",
    "        new_list = []\n",
    "        # Iterate over the splitted sentences\n",
    "        for s in splitted_sentences:\n",
    "            # Lowerize characters\n",
    "            for key, value in lower_chars_dict.items():\n",
    "                s = s.replace(key, value)\n",
    "            new_list.append(s)\n",
    "        processed_sentences.extend(new_list)\n",
    "    print(processed_sentences)\n",
    "    \n",
    "    diactrize_labels = processed_sentences.copy()\n",
    "    # Iterate over the sentences to diacritize them\n",
    "    for i, sentence in enumerate(processed_sentences):\n",
    "        diactrize_labels[i] = map_diacritics(list(sentence))\n",
    "        # Add padding to the diacritized sentence\n",
    "        diactrize_labels[i] = padding(diactrize_labels[i], 0, 100)\n",
    "    # Asci sentences \n",
    "    asci_sentences = processed_sentences.copy()\n",
    "    # Iterate over the sentences to convert them to asci and map them to numbers\n",
    "    for i, sentence in enumerate(processed_sentences):\n",
    "        asci_sentences[i] = convert_to_ascii(sentence)\n",
    "        \n",
    "    numeric_sentences = []\n",
    "    for i, sentence in enumerate(asci_sentences):\n",
    "        numeric_sentence = []\n",
    "        for char in sentence:\n",
    "            numeric_sentence.append(characters_dictionary[char])\n",
    "        # Add padding to the numeric sentence\n",
    "        numeric_sentence = padding(numeric_sentence, 0, 100)\n",
    "        numeric_sentences.append(numeric_sentence)\n",
    "    return processed_sentences, diactrize_labels, asci_sentences, numeric_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkish_sentences = [\n",
    "    \"uzun zamandır görüşemedik, nasılsın?\",\n",
    "    \"bu nasıl bir soru?\",\n",
    "    \"Bu cümle çok uzun bir cümle olacak ve bu cümlede çok fazla kelime olacak. Devam ediyoruz çünkü bu cümle hala bitmedi. Bitmedi bitmeyecek. Çünkü bu cümle çok uzun bir cümle olacak ve bu cümlede çok fazla kelime olacak. Devam ediyoruz çünkü bu cümle hala bitmedi. Bitmedi bitmeyecek. Devam et\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['uzun zamandır görüşemedik, nasılsın?', 'bu nasıl bir soru?', 'Bu cümle çok uzun bir cümle olacak ve bu cümlede çok fazla kelime olacak. Devam ediyoruz çünkü bu cümle hala bitmedi. Bitmedi bitmeyecek. Çünkü bu cümle çok uzun bir cümle olacak ve bu cümlede çok fazla kelime olacak. Devam ediyoruz çünkü bu cümle hala bitmedi. Bitmedi bitmeyecek. Devam et']\n",
      "['uzun zamandır görüşemedik nasılsın', 'bu nasıl bir soru', 'bu cümle çok uzun bir cümle olacak ve bu cümlede çok fazla kelime olacak devam ediyoruz çünkü bu', 'cümle hala bitmedi bitmedi bitmeyecek çünkü bu cümle çok uzun bir cümle olacak ve bu cümlede çok', 'fazla kelime olacak devam ediyoruz çünkü bu cümle hala bitmedi bitmedi bitmeyecek devam et']\n"
     ]
    }
   ],
   "source": [
    "# Raw sentences\n",
    "raw_sentences = nlp_train[\"Sentence\"].values\n",
    "\n",
    "processed_turkish_sentences,labels,asci_sentences,numeric_sentences = prepare_train_dataset(turkish_sentences.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiacritizationBiLSTMCRF(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels):\n",
    "        super(DiacritizationBiLSTMCRF, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim // 2, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, sentences, labels=None):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(sentences)\n",
    "        \n",
    "        # BiLSTM layer\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        emissions = self.fc(x)\n",
    "        # CRF layer\n",
    "        if labels is not None:\n",
    "            # If labels are provided, calculate the loss\n",
    "            loss = -self.crf(emissions, labels)\n",
    "            return loss\n",
    "        else:\n",
    "            # Otherwise, return the best path\n",
    "            prediction = self.crf.decode(emissions)\n",
    "            # get probabilites of the best path\n",
    "            return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiacritizationBiLSTMCRF(\n",
       "  (embedding): Embedding(34, 128)\n",
       "  (lstm): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=4, bias=True)\n",
       "  (crf): CRF(num_tags=4)\n",
       ")"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of initializing the model\n",
    "vocab_size = len(characters_dictionary) + 1  # Number of unique characters in your character dictionary + 1 for padding\n",
    "embed_dim = 128\n",
    "hidden_dim = 256\n",
    "num_labels = 4  # Diacritic or not\n",
    "model = DiacritizationBiLSTMCRF(vocab_size, embed_dim, hidden_dim, num_labels)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_sentences_tensor = torch.tensor(numeric_sentences, dtype=torch.long)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Splitting three variables: numeric_sentences_tensor, labels_tensor, and another_tensor\n",
    "X_train, X_val, y_train, y_val, sentences_train, sentences_val = train_test_split(\n",
    "    numeric_sentences_tensor, labels_tensor, asci_sentences, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create TensorDataset and DataLoader for training and validation\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "def train_model(model, data_loader, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for sentences, label_seqs in data_loader:\n",
    "            sentences, label_seqs = sentences.to(device), label_seqs.to(device)\n",
    "            \n",
    "            # Forward pass: Compute predicted y by passing x to the model\n",
    "            loss = model(sentences, labels=label_seqs)\n",
    "            \n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "#train_model(model, train_loader, optimizer, num_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_sentence(sentence):\n",
    "    print(sentence)\n",
    "    # sentence = [sentence]\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = remove_puncutations_and_numbers(sentence)\n",
    "    print(sentence)\n",
    "    \n",
    "    # Lowerize characters\n",
    "    for key, value in lower_chars_dict.items():\n",
    "        sentence = sentence.replace(key, value)\n",
    "    print(sentence)\n",
    "    \n",
    "    # Convert to asci\n",
    "    sentence = convert_to_ascii(sentence)\n",
    "    print(sentence)\n",
    "    \n",
    "    sentences_array = []\n",
    "    # Split sentence into smaller sentences by using split_sentences function\n",
    "    sentences_array.extend(split_sentences([sentence]))\n",
    "    print(sentences_array)\n",
    "    # Map to numbers\n",
    "    numeric_sentences = []\n",
    "    for sentence in sentences_array:\n",
    "        numeric_sentence = []\n",
    "        print(len(sentence))\n",
    "        for char in sentence:\n",
    "            numeric_sentence.append(characters_dictionary[char])\n",
    "        numeric_sentences.append(numeric_sentence)\n",
    "        \n",
    "    \n",
    "    # Add padding\n",
    "    # print(numeric_sentence)\n",
    "    return numeric_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bu cümle çok uzun bir cümle olacak ve bu cümlede çok fazla kelime olacak. Devam ediyoruz çünkü bu cümle hala bitmedi. Bitmedi bitmeyecek. Çünkü bu cümle çok uzun bir cümle olacak ve bu cümlede çok fazla kelime olacak. Devam ediyoruz çünkü bu cümle hala bitmedi. Bitmedi bitmeyecek. Devam et. Devam et\n",
      "Bu cümle çok uzun bir cümle olacak ve bu cümlede çok fazla kelime olacak Devam ediyoruz çünkü bu cümle hala bitmedi Bitmedi bitmeyecek Çünkü bu cümle çok uzun bir cümle olacak ve bu cümlede çok fazla kelime olacak Devam ediyoruz çünkü bu cümle hala bitmedi Bitmedi bitmeyecek Devam et Devam et\n",
      "bu cümle çok uzun bir cümle olacak ve bu cümlede çok fazla kelime olacak devam ediyoruz çünkü bu cümle hala bitmedi bitmedi bitmeyecek çünkü bu cümle çok uzun bir cümle olacak ve bu cümlede çok fazla kelime olacak devam ediyoruz çünkü bu cümle hala bitmedi bitmedi bitmeyecek devam et devam et\n",
      "bu cumle cok uzun bir cumle olacak ve bu cumlede cok fazla kelime olacak devam ediyoruz cunku bu cumle hala bitmedi bitmedi bitmeyecek cunku bu cumle cok uzun bir cumle olacak ve bu cumlede cok fazla kelime olacak devam ediyoruz cunku bu cumle hala bitmedi bitmedi bitmeyecek devam et devam et\n",
      "['bu cumle cok uzun bir cumle olacak ve bu cumlede cok fazla kelime olacak devam ediyoruz cunku bu', 'cumle hala bitmedi bitmedi bitmeyecek cunku bu cumle cok uzun bir cumle olacak ve bu cumlede cok', 'fazla kelime olacak devam ediyoruz cunku bu cumle hala bitmedi bitmedi bitmeyecek devam et devam et']\n",
      "96\n",
      "96\n",
      "99\n",
      "[[2, 25, 33, 3, 25, 16, 15, 6, 33, 3, 18, 14, 33, 25, 29, 25, 17, 33, 2, 12, 21, 33, 3, 25, 16, 15, 6, 33, 18, 15, 1, 3, 1, 14, 33, 27, 6, 33, 2, 25, 33, 3, 25, 16, 15, 6, 5, 6, 33, 3, 18, 14, 33, 7, 1, 29, 15, 1, 33, 14, 6, 15, 12, 16, 6, 33, 18, 15, 1, 3, 1, 14, 33, 5, 6, 27, 1, 16, 33, 6, 5, 12, 28, 18, 21, 25, 29, 33, 3, 25, 17, 14, 25, 33, 2, 25], [3, 25, 16, 15, 6, 33, 10, 1, 15, 1, 33, 2, 12, 24, 16, 6, 5, 12, 33, 2, 12, 24, 16, 6, 5, 12, 33, 2, 12, 24, 16, 6, 28, 6, 3, 6, 14, 33, 3, 25, 17, 14, 25, 33, 2, 25, 33, 3, 25, 16, 15, 6, 33, 3, 18, 14, 33, 25, 29, 25, 17, 33, 2, 12, 21, 33, 3, 25, 16, 15, 6, 33, 18, 15, 1, 3, 1, 14, 33, 27, 6, 33, 2, 25, 33, 3, 25, 16, 15, 6, 5, 6, 33, 3, 18, 14], [7, 1, 29, 15, 1, 33, 14, 6, 15, 12, 16, 6, 33, 18, 15, 1, 3, 1, 14, 33, 5, 6, 27, 1, 16, 33, 6, 5, 12, 28, 18, 21, 25, 29, 33, 3, 25, 17, 14, 25, 33, 2, 25, 33, 3, 25, 16, 15, 6, 33, 10, 1, 15, 1, 33, 2, 12, 24, 16, 6, 5, 12, 33, 2, 12, 24, 16, 6, 5, 12, 33, 2, 12, 24, 16, 6, 28, 6, 3, 6, 14, 33, 5, 6, 27, 1, 16, 33, 6, 24, 33, 5, 6, 27, 1, 16, 33, 6, 24]]\n"
     ]
    }
   ],
   "source": [
    "uzun_cumle = \"Bu cümle çok uzun bir cümle olacak ve bu cümlede çok fazla kelime olacak. Devam ediyoruz çünkü bu cümle hala bitmedi. Bitmedi bitmeyecek. Çünkü bu cümle çok uzun bir cümle olacak ve bu cümlede çok fazla kelime olacak. Devam ediyoruz çünkü bu cümle hala bitmedi. Bitmedi bitmeyecek. Devam et. Devam et\"\n",
    "new_sentence = predict_test_sentence(uzun_cumle)\n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, characters_dictionary, max_length=200):\n",
    "    # Remove punctuation and numbers\n",
    "    sentence = ''.join([i for i in sentence if not i.isdigit() and i not in string.punctuation])\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    # Convert to numeric form using the character dictionary\n",
    "    numeric_sentence = [characters_dictionary.get(char, 0) for char in sentence]  # default to 0 if char not found\n",
    "    \n",
    "    # Padding\n",
    "    if len(numeric_sentence) > max_length:\n",
    "        numeric_sentence = numeric_sentence[:max_length]\n",
    "    else:\n",
    "        numeric_sentence += [0] * (max_length - len(numeric_sentence))\n",
    "    \n",
    "    return torch.tensor([numeric_sentence], dtype=torch.long).to(device)\n",
    "\n",
    "def predict(model, sentence, characters_dictionary):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        numeric_sentence = preprocess_sentence(sentence, characters_dictionary)\n",
    "        prediction = model(numeric_sentence)\n",
    "        return prediction[0]  # since we have a batch size of 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 3, 3, 3, 2, 0, 3, 1, 3, 0, 1, 2, 3, 2, 2, 3, 3, 1, 1, 0, 1, 3, 3, 1, 3, 3, 2, 0, 3, 1, 3, 1, 3, 3, 1, 3, 2, 3, 0, 0, 1, 3, 1, 3, 1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "imrali Mit gorusmesi ihtiyac duyuldukca 12452 oluyor\n",
      "Original: imrali Mit gorusmesi ihtiyac duyuldukca 12452 oluyor\n",
      "Processed: imralı Mit görüşmesi ihtiyaç duyuldukça 12452 oluyor\n"
     ]
    }
   ],
   "source": [
    "# Example sentence\n",
    "test_sentence = \"imrali Mit gorusmesi ihtiyac duyuldukca 12452 oluyor\"\n",
    "model = DiacritizationBiLSTMCRF(vocab_size, embed_dim, hidden_dim, num_labels)\n",
    "model.to(device)\n",
    "# Load the saved weights\n",
    "model.load_state_dict(torch.load('/Users/mustafa/Desktop/Courses/Natural Language Processing/Project/Code/YZV405_2324_150200326_150210339/model_epoch_last.pth', map_location=torch.device('cpu')))\n",
    "# Predict using the model\n",
    "predicted_labels = predict(model, test_sentence, characters_dictionary)\n",
    "print(predicted_labels)\n",
    "\n",
    "# Define labels to diacritics mapping\n",
    "def labels_to_diacritics(text_sentence, labels):\n",
    "    print(text_sentence)\n",
    "    diacritics_map_dict = {\"i\": \"ı\", \"o\": \"ö\", \"u\": \"ü\", \"g\": \"ğ\", \"s\": \"ş\", \"c\": \"ç\", \"I\": \"İ\"}\n",
    "    output_sentence = \"\"\n",
    "    for i in range(len(text_sentence)):\n",
    "        if labels[i] == 2:\n",
    "            if text_sentence[i] in diacritics_map_dict:\n",
    "                output_sentence += diacritics_map_dict[text_sentence[i]]\n",
    "            else:\n",
    "                output_sentence += text_sentence[i]\n",
    "        else:\n",
    "            output_sentence += text_sentence[i]\n",
    "    return output_sentence\n",
    "\n",
    "# Show the results\n",
    "output_sentence = labels_to_diacritics(test_sentence, predicted_labels)\n",
    "print(\"Original:\", test_sentence)\n",
    "print(\"Processed:\", output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_overall(test_result, testgold):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  print(test_result)\n",
    "  print(testgold)\n",
    "  # count number of correctly diacritized words\n",
    "  for i in range(len(testgold)):\n",
    "    for m in range(len(testgold[i].split())):\n",
    "      if test_result[i].split()[m] == testgold[i].split()[m]:\n",
    "        correct += 1\n",
    "      total +=1\n",
    "\n",
    "  return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "diacritic_versions = {\"i\":\"ı\", \"ı\":\"i\", \"o\":\"ö\", \"ö\":\"o\", \"u\":\"ü\", \"ü\":\"u\", \"g\":\"ğ\", \"ğ\":\"g\", \"s\":\"ş\", \"ş\":\"s\", \"c\":\"ç\", \"ç\":\"c\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_morphology(sentences):\n",
    "    morphology = TurkishMorphology.create_with_defaults()\n",
    "    morphology.ignoreDiacriticsInAnalysis = False\n",
    "    spellChecker = TurkishSpellChecker(morphology)\n",
    "\n",
    "    corrected_sentences = sentences.copy()\n",
    "\n",
    "    # Iterate through each sentence\n",
    "    for i in range(len(corrected_sentences)):\n",
    "        # Split the sentence into words and iterate through each word\n",
    "        words = corrected_sentences[i].split()\n",
    "        for m in range(len(words)):\n",
    "            analysis = morphology.analyze(words[m])\n",
    "\n",
    "            # Check if the word has no analysis results\n",
    "            if len(analysis.analysis_results) == 0:\n",
    "                print(\"No analysis results for:\", words[m])\n",
    "                \n",
    "                # Check if there are suggestions from the spell checker\n",
    "                suggestions = spellChecker.suggest_for_word(words[m])\n",
    "                if len(suggestions) != 0:\n",
    "                    for suggested_word in suggestions:\n",
    "                        print(\"Suggested word:\", suggested_word)                  \n",
    "                        # if the suggested word has the same length as the original word, and just \"ıioöuügğsşcç\" characters are different, replace the word\n",
    "                        if len(suggested_word) == len(words[m]):\n",
    "                            for char1, char2 in zip(suggested_word, words[m]):\n",
    "                                if char1 in diacritic_versions and char2 == diacritic_versions[char1]:\n",
    "                                    continue\n",
    "                                if char1 in diacritic_versions and char2 == char1:\n",
    "                                    continue\n",
    "                                if char1 not in diacritic_versions and char1 == char2:\n",
    "                                    continue\n",
    "                                elif char1 in diacritic_versions and char2 != diacritic_versions[char1] and char2 != char1:\n",
    "                                    break\n",
    "                                elif char1 not in diacritic_versions and char1 != char2:\n",
    "                                    break\n",
    "                            else:\n",
    "                                print(\"Suggested word is approved:\", suggested_word)\n",
    "                                words[m] = suggested_word\n",
    "                                break\n",
    "\n",
    "        # Join the modified words back into a sentence\n",
    "        corrected_sentences[i] = ' '.join(words)\n",
    "\n",
    "    return corrected_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, val_loader, characters_dictionary):\n",
    "    model.eval()\n",
    "    predicted_results = []\n",
    "    true_results = []\n",
    "\n",
    "    for (sentences, label_seqs), real_sentence in zip(val_loader, sentences_val):\n",
    "        sentence = sentences.squeeze(0)  # Remove the batch dimension\n",
    "        sentence = sentence.tolist()  # Convert tensor to list of sentences\n",
    "        print(sentence)\n",
    "        # Preprocess each sentence and obtain predicted labels\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            prediction = model(sentences)\n",
    "        predicted_sentence = labels_to_diacritics(real_sentence, prediction[0])\n",
    "        \n",
    "        # Append predicted and true results\n",
    "        predicted_results.append(predicted_sentence)\n",
    "        true_results.append(labels_to_diacritics(real_sentence, label_seqs.squeeze(0).tolist()))\n",
    "\n",
    "    last_version = check_morphology(predicted_results)\n",
    "    print(\"Last version:\", last_version)\n",
    "    # Calculate accuracy\n",
    "    accuracy = acc_overall(last_version, true_results)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 22, 1, 21, 12, 15, 12, 33, 2, 12, 21, 33, 22, 24, 1, 17, 5, 1, 21, 24, 33, 18, 15, 25, 22, 24, 25, 21, 16, 25, 22, 24, 25, 21, 33, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "basarili bir standart olusturmustur \n",
      "basarili bir standart olusturmustur \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-06 21:37:20,989 - zemberek.morphology.turkish_morphology - INFO\n",
      "Msg: TurkishMorphology instance initialized in 4.086855888366699\n",
      "\n",
      "Last version: ['başarılı bir standart oluşturmuştur']\n",
      "['başarılı bir standart oluşturmuştur']\n",
      "['başarılı bir standart oluşturmuştur ']\n",
      "Validation Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Test the model on the validation dataset\n",
    "accuracy = test_model(model, val_loader, characters_dictionary)\n",
    "print(\"Validation Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['Çok uzun bir cümle olacak ve bu cümleyi parçalara ayırmamız gerekecek.'\n",
      " 'Çünkü modelimiz çok uzun cümleleri işleyemiyor. Bu yüzden cümleyi parçalara ayırıp işleyeceğiz.'\n",
      " 'Bu işlem sonucunda cümledeki kelimelerin üzerindeki ağırlıkların doğru tahmin edilmesini bekliyoruz.'\n",
      " 'Modelimizin bu işlemi başarılı bir şekilde yapacağına inanıyoruz. Umarım sonuçlar bizi yanıltmaz.'\n",
      " 'Çünkü bu sonuçlar bizim için çok önemli.']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "def predict_test(model, test_sentences, characters_dictionary, max_length=100):\n",
    "    model.eval()\n",
    "    predicted_results = []\n",
    "    \n",
    "\n",
    "    for sentence in test_sentences:\n",
    "        # Store the original sentence\n",
    "        original_sentence = sentence\n",
    "        \n",
    "        sentence = [\"Çok uzun bir cümle olacak ve bu cümleyi parçalara ayırmamız gerekecek. Çünkü modelimiz çok uzun cümleleri işleyemiyor. Bu yüzden cümleyi parçalara ayırıp işleyeceğiz. Bu işlem sonucunda cümledeki kelimelerin üzerindeki ağırlıkların doğru tahmin edilmesini bekliyoruz. Modelimizin bu işlemi başarılı bir şekilde yapacağına inanıyoruz. Umarım sonuçlar bizi yanıltmaz. Çünkü bu sonuçlar bizim için çok önemli. \"]\n",
    "        # Split long sentences into manageable parts by using split_sentences function\n",
    "        print(len(sentence))\n",
    "        parts = split_sentences(sentence, max_length)\n",
    "        # \n",
    "        print(parts)\n",
    "        break\n",
    "        \n",
    "        predicted_sentence = \"\"\n",
    "\n",
    "        for part in parts:\n",
    "            # Preprocess and predict for each part\n",
    "            numeric_sentence = preprocess_sentence(part, characters_dictionary, max_length)\n",
    "            numeric_part = torch.tensor([numeric_sentence], dtype=torch.long).to(device)\n",
    "            prediction = predict(model, numeric_part, characters_dictionary)\n",
    "            # Apply diacritics to each part and combine\n",
    "            predicted_part = labels_to_diacritics(part, prediction)\n",
    "            predicted_sentence += predicted_part\n",
    "\n",
    "        predicted_results.append(predicted_sentence)\n",
    "\n",
    "    return predicted_results\n",
    "\n",
    "# Function to preprocess and convert a sentence to its numeric form\n",
    "def preprocess_sentence(sentence, characters_dictionary, max_length):\n",
    "    numeric_sentence = [characters_dictionary.get(char, 0) for char in sentence.lower()]\n",
    "    # Padding\n",
    "    if len(numeric_sentence) > max_length:\n",
    "        numeric_sentence = numeric_sentence[:max_length]\n",
    "    else:\n",
    "        numeric_sentence += [0] * (max_length - len(numeric_sentence))\n",
    "    return numeric_sentence\n",
    "\n",
    "# Helper function to handle predictions for a numeric sentence\n",
    "def predict(model, numeric_sentence, characters_dictionary):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        prediction = model(numeric_sentence)\n",
    "    return prediction[0]  # Assuming batch size of 1 for simplicity\n",
    "\n",
    "\n",
    "# Make predictions on the test set\n",
    "test_sentences = nlp_test[\"Sentence\"].values\n",
    "predicted_results = predict_test(model, test_sentences, characters_dictionary)\n",
    "print(predicted_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "['Çok uzun bir cümle olacak ve bu cümleyi parçalara ayırmamız gerekecek.'\n",
      " 'Çünkü modelimiz çok uzun cümleleri işleyemiyor. Bu yüzden cümleyi parçalara ayırıp işleyeceğiz.'\n",
      " 'Bu işlem sonucunda cümledeki kelimelerin üzerindeki ağırlıkların doğru tahmin edilmesini bekliyoruz.'\n",
      " 'Modelimizin bu işlemi başarılı bir şekilde yapacağına inanıyoruz. Umarım sonuçlar bizi yanıltmaz.'\n",
      " 'Çünkü bu sonuçlar bizim için çok önemli.']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "test_sentences = nlp_test[\"Sentence\"].values\n",
    "predicted_results = predict_test(model, test_sentences, characters_dictionary)\n",
    "print(predicted_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predicted_results)):\n",
    "    # Check if length of the predicted sentence is different than the original sentence\n",
    "    if len(predicted_results[i]) != len(test_sentences[i]):\n",
    "        print(\"Original:\", test_sentences[i])\n",
    "        print(\"Predicted:\", predicted_results[i])\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[408], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the predictions to a CSV file, it will have two columns: \"ID\" and \"Sentence\"\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m output_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mID\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mnlp_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSentence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_results\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_project/lib/python3.11/site-packages/pandas/core/frame.py:778\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    772\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    773\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    774\u001b[0m     )\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 778\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    779\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    780\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_project/lib/python3.11/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_project/lib/python3.11/site-packages/pandas/core/internals/construction.py:114\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         index \u001b[38;5;241m=\u001b[39m \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m         index \u001b[38;5;241m=\u001b[39m ensure_index(index)\n",
      "File \u001b[0;32m~/miniconda3/envs/nlp_project/lib/python3.11/site-packages/pandas/core/internals/construction.py:677\u001b[0m, in \u001b[0;36m_extract_index\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    675\u001b[0m lengths \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll arrays must be of the same length\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[1;32m    680\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    681\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    682\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "# Save the predictions to a CSV file, it will have two columns: \"ID\" and \"Sentence\"\n",
    "output_df = pd.DataFrame({\"ID\": nlp_test.index, \"Sentence\": predicted_results})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to a CSV file\n",
    "output_df.to_csv(\"predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tr ekonomi ve politika haberleri türkiye nin ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>üye girişi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>son güncelleme 12:12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Imralı Mit görüşmesi ihtiyaç duyuldukça oluyor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Suriye deki silahlı selefi muhalifler yeni ku...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1152</th>\n",
       "      <td>1152</td>\n",
       "      <td>Yüreğir Adana ilimize ait şırın bir ilçedir</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1153</th>\n",
       "      <td>1153</td>\n",
       "      <td>yüze gülüçülüğün at oynattığı bir aydınlar ort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>1154</td>\n",
       "      <td>zavallı adamı oracıkta astılar ve hiç kimse se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>1155</td>\n",
       "      <td>zengin çocuklarına arız münasebetsizlikler fak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>1156</td>\n",
       "      <td>senin acın hepimizin açısıdır</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1157 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID                                           Sentence\n",
       "0        0   tr ekonomi ve politika haberleri türkiye nin ...\n",
       "1        1                                        üye girişi \n",
       "2        2                              son güncelleme 12:12 \n",
       "3        3    Imralı Mit görüşmesi ihtiyaç duyuldukça oluyor \n",
       "4        4   Suriye deki silahlı selefi muhalifler yeni ku...\n",
       "...    ...                                                ...\n",
       "1152  1152       Yüreğir Adana ilimize ait şırın bir ilçedir \n",
       "1153  1153  yüze gülüçülüğün at oynattığı bir aydınlar ort...\n",
       "1154  1154  zavallı adamı oracıkta astılar ve hiç kimse se...\n",
       "1155  1155  zengin çocuklarına arız münasebetsizlikler fak...\n",
       "1156  1156                     senin acın hepimizin açısıdır \n",
       "\n",
       "[1157 rows x 2 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
