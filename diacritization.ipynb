{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, BertModel, AutoConfig, AutoTokenizer\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import time;\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchcrf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-crf\n",
    "# !pip install unidecode\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    \n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_train=pd.read_csv(\"/kaggle/input/nlp-project-train/train.csv\", index_col=[0])\n",
    "# nlp_test=pd.read_csv(\"/kaggle/input/nlp-project-train/test.csv\",index_col=[0],encoding=\"windows-1252\") \n",
    "\n",
    "nlp_train=pd.read_csv(\"train.csv\", index_col=[0])\n",
    "nlp_test=pd.read_csv(\"test.csv\",index_col=[0],encoding=\"windows-1252\") \n",
    "nlp_train = nlp_train[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ascii(sentence):\n",
    "    text = unidecode(sentence)\n",
    "    return text\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "        text = text.replace('  ', ' ')\n",
    "    return text.strip()\n",
    "\n",
    "def split_sentences(sentences, max_length=100):\n",
    "    punctuations = {'.', ',', ';', ':'}\n",
    "    results = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        while len(sentence) > max_length:\n",
    "            # find last punctuation before max_length\n",
    "            split_pos = -1\n",
    "            for p in punctuations:\n",
    "                pos = sentence.rfind(p, 0, max_length + 1)\n",
    "                if pos > split_pos:\n",
    "                    split_pos = pos\n",
    "            \n",
    "            # If no punctuation found, split at the last space before max_length\n",
    "            if split_pos == -1:\n",
    "                split_pos = sentence.rfind(' ', 0, max_length + 1)\n",
    "            \n",
    "            # If no space found, just split at max_length\n",
    "            if split_pos == -1:\n",
    "                split_pos = max_length\n",
    "            \n",
    "            # Append the split segment to results\n",
    "            results.append(sentence[:split_pos + 1].strip())\n",
    "            # Move the rest of the sentence forward\n",
    "            sentence = sentence[split_pos + 1:].strip()\n",
    "        \n",
    "        # Append the remainder of the sentence if it's not empty\n",
    "        if sentence:\n",
    "            results.append(sentence)\n",
    "    \n",
    "    return np.array(results)\n",
    "\n",
    "\n",
    "def encode_word(sentence):\n",
    "    words = sentence.split()\n",
    "    encoded_sentence = []\n",
    "    for word in words:\n",
    "        encoded_word = []\n",
    "        for char in word:\n",
    "            # print(char)\n",
    "            if char in \"ıöüğşç\":\n",
    "                encoded_word.append(2)\n",
    "            else:\n",
    "                encoded_word.append(1)\n",
    "            # print(encoded_word)\n",
    "        encoded_sentence.append(encoded_word)\n",
    "    return encoded_sentence\n",
    "\n",
    "\n",
    "\n",
    "def character_tokenizer(text):\n",
    "    return [c if c != ' ' else '[SPACE]' for c in text]\n",
    "\n",
    "def padding(text, filling_char, max_length):\n",
    "    if type(text) is str:\n",
    "        text = text + filling_char * (max_length - len(text))\n",
    "    elif type(text) is list:\n",
    "        text = text + [filling_char] * (max_length - len(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Preprocess***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw sentences\n",
    "raw_sentences = nlp_train[\"Sentence\"].values\n",
    "# Split sentences\n",
    "sentences = split_sentences(raw_sentences, max_length=200)\n",
    "# Remove punctuations\n",
    "sentences = [remove_punctuations(s).lower() for s in sentences]\n",
    "nlp_train[\"Label\"] = nlp_train[\"Sentence\"]\n",
    "# Apply convert to ascii to y_train\n",
    "asci_sentences = [convert_to_ascii(s) for s in sentences]\n",
    "\n",
    "sentences_diacritics = sentences.copy()\n",
    "for sentence_index in range(len(sentences_diacritics)):\n",
    "    sentence = sentences_diacritics[sentence_index]\n",
    "    new_sentence = encode_word(sentence)\n",
    "    sentences_diacritics[sentence_index] = new_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Split sentences to lists of characters***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences_splitted = [character_tokenizer(s) for s in sentences]\n",
    "\n",
    "# asci_sentences_splitted = [character_tokenizer(s) for s in asci_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3825, 8725, 1992, 2416, 4456, 24513, 2525, 5292, 5953, 4165, 1996, 5538, 27202, 2293, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "sınıf , havuz ve açık deniz çalışmalarıyla , tüm dünyada geçerli , başarılı bir standart oluşturmuştur . \n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "indices=tokenizer.batch_encode_plus(sentences,max_length=200,add_special_tokens=True, return_attention_mask=True,padding=True,truncation=True)\n",
    "input_ids=indices[\"input_ids\"]\n",
    "attention_masks=indices[\"attention_mask\"]\n",
    "print(input_ids[0])\n",
    "print(raw_sentences[0])\n",
    "print(attention_masks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sentences_diacritics)):\n",
    "    for j in range(len(sentences_diacritics[i])):\n",
    "        sentences_diacritics[i][j] = padding(sentences_diacritics[i][j], 0, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dictionary to store inputs and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to store inputs and labels\n",
    "inputs = {\n",
    "    \"input_ids\": input_ids,\n",
    "    \"attention_mask\": attention_masks,\n",
    "    \"labels\": sentences_diacritics\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 2, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [2, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 2, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 2, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 2, 1, 1, 2, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "print(inputs[\"labels\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_and_pad_labels(labels, pad_value=0, max_length=None):\n",
    "    # If max_length is not specified, find the maximum length of any label sequence in the dataset\n",
    "    if max_length is None:\n",
    "        max_length = max(len(label) for sentence_labels in labels for label in sentence_labels)\n",
    "    \n",
    "    padded_labels = []\n",
    "    for sentence_labels in labels:\n",
    "        flat_labels = [label for word_labels in sentence_labels for label in word_labels]\n",
    "        # Pad the flattened label sequence if it's shorter than max_length\n",
    "        if len(flat_labels) < max_length:\n",
    "            flat_labels += [pad_value] * (max_length - len(flat_labels))\n",
    "        padded_labels.append(flat_labels[:max_length])  # Ensure the length matches max_length exactly\n",
    "    return padded_labels\n",
    "\n",
    "# Calculate the maximum sequence length from your input_ids to ensure label alignment\n",
    "max_seq_length = max(len(seq) for seq in input_ids)\n",
    "\n",
    "# Flatten and pad the labels\n",
    "flat_padded_labels = flatten_and_pad_labels(inputs[\"labels\"], pad_value=0, max_length=max_seq_length)\n",
    "\n",
    "# Convert labels to tensor\n",
    "labels_tensor = torch.tensor(flat_padded_labels, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the input data and labels into torch tensors\n",
    "input_ids = torch.tensor(inputs[\"input_ids\"])\n",
    "attention_masks = torch.tensor(inputs[\"attention_mask\"])\n",
    "labels = labels_tensor\n",
    "\n",
    "# Create TensorDataset\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 32  # you can adjust this size depending on your GPU capacity\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=RandomSampler(train_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    val_dataset,\n",
    "    sampler=SequentialSampler(val_dataset),\n",
    "    batch_size=batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiacritizationModel(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (lstm): LSTM(768, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=3, bias=True)\n",
       "  (crf): CRF(num_tags=3)\n",
       ")"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DiacritizationModel(nn.Module):\n",
    "    def __init__(self, num_labels):\n",
    "        super(DiacritizationModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n",
    "        self.lstm = nn.LSTM(input_size=768, hidden_size=256, num_layers=2, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(512, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        sequence_output = outputs[0]\n",
    "        lstm_output, _ = self.lstm(sequence_output)\n",
    "        logits = self.fc(lstm_output)\n",
    "        \n",
    "        if labels is not None:\n",
    "            # Only return the loss if labels are provided\n",
    "            loss = -self.crf(logits, labels, mask=attention_mask.byte())\n",
    "            return loss\n",
    "        else:\n",
    "            # Return logits for decoding\n",
    "            return logits\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = DiacritizationModel(num_labels=3)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "# Total number of training steps is the number of batches * number of epochs.\n",
    "total_steps = len(train_dataloader) * num_epochs\n",
    "\n",
    "# Set up the learning rate scheduler\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps=0,\n",
    "                                            num_training_steps=total_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Loss: 508.00115966796875\n",
      "Validation Loss: 194.9593963623047\n",
      "Epoch 2/10 | Loss: 433.62608846028644\n",
      "Validation Loss: 170.79214477539062\n",
      "Epoch 3/10 | Loss: 385.9591471354167\n",
      "Validation Loss: 154.65484619140625\n",
      "Epoch 4/10 | Loss: 353.9159240722656\n",
      "Validation Loss: 146.4486846923828\n",
      "Epoch 5/10 | Loss: 335.6189676920573\n",
      "Validation Loss: 141.2999725341797\n",
      "Epoch 6/10 | Loss: 321.75197347005206\n",
      "Validation Loss: 136.6573028564453\n",
      "Epoch 7/10 | Loss: 309.0523376464844\n",
      "Validation Loss: 132.35865783691406\n",
      "Epoch 8/10 | Loss: 298.2560221354167\n",
      "Validation Loss: 128.31820678710938\n",
      "Epoch 9/10 | Loss: 289.38800048828125\n",
      "Validation Loss: 125.27009582519531\n",
      "Epoch 10/10 | Loss: 284.2415059407552\n",
      "Validation Loss: 124.18313598632812\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_dataloader, validation_dataloader, epochs, optimizer, scheduler, device):\n",
    "    # Store the average loss after each epoch so we can plot them.\n",
    "    loss_values = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "            b_input_ids = batch[0].to(device)\n",
    "            b_input_mask = batch[1].to(device)\n",
    "            b_labels = batch[2].to(device)\n",
    "\n",
    "            # Clear previously calculated gradients\n",
    "            model.zero_grad()\n",
    "\n",
    "            # Forward pass (calculate current loss)\n",
    "            loss = model(b_input_ids, b_input_mask, b_labels)\n",
    "\n",
    "            # Accumulate the training loss over all batches\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Perform a backward pass to calculate gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            # Update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate\n",
    "            scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over the training data.\n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        loss_values.append(avg_train_loss)\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs} | Loss: {avg_train_loss}')\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        eval_accuracy = 0\n",
    "        nb_eval_steps = 0\n",
    "\n",
    "        for batch in validation_dataloader:\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Forward pass, calculate logit predictions\n",
    "                loss = model(b_input_ids, b_input_mask, b_labels)\n",
    "\n",
    "            # Accumulate validation loss\n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        print(f'Validation Loss: {eval_loss / nb_eval_steps}')\n",
    "\n",
    "    return loss_values\n",
    "\n",
    "\n",
    "\n",
    "# Call the training function\n",
    "loss_values = train(model, train_dataloader, validation_dataloader, num_epochs, optimizer, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # Remove punctuations and convert to lowercase\n",
    "    sentence = remove_punctuations(sentence).lower()\n",
    "    # Convert to ASCII if needed\n",
    "    sentence = convert_to_ascii(sentence)\n",
    "    return sentence\n",
    "\n",
    "def prepare_input(tokenizer, sentence, device):\n",
    "    # Tokenize the sentence\n",
    "    inputs = tokenizer.encode_plus(\n",
    "        sentence,\n",
    "        None,\n",
    "        add_special_tokens=True,\n",
    "        max_length=200,\n",
    "        padding='max_length',\n",
    "        return_token_type_ids=False,\n",
    "        return_attention_mask=True,\n",
    "        truncation=True\n",
    "    )\n",
    "    # Convert to tensors\n",
    "    input_ids = torch.tensor(inputs['input_ids']).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    attention_mask = torch.tensor(inputs['attention_mask']).unsqueeze(0).to(device)  # Add batch dimension\n",
    "    return input_ids, attention_mask\n",
    "\n",
    "def predict(model, tokenizer, sentence, device):\n",
    "    model.eval()\n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    input_ids, attention_mask = prepare_input(tokenizer, preprocessed_sentence, device)\n",
    "    with torch.no_grad():\n",
    "        # Get model logits\n",
    "        logits = model(input_ids, attention_mask)\n",
    "        # Decode the predictions using CRF layer\n",
    "        predictions = model.crf.decode(logits)\n",
    "    return predictions[0]  # Remove the batch dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_diacritics(text, predictions):\n",
    "    diacritics_map = {\n",
    "        'u': {2: 'ü'},\n",
    "        'o': {2: 'ö'},\n",
    "        'i': {2: 'ı'},\n",
    "        'g': {2: 'ğ'},\n",
    "        's': {2: 'ş'},\n",
    "        'c': {2: 'ç'}\n",
    "    }\n",
    "    result = []\n",
    "    for i, char in enumerate(text):\n",
    "        if char in diacritics_map and predictions[i] in diacritics_map[char]:\n",
    "            print(char)\n",
    "            # Apply the diacritic transformation\n",
    "            result.append(diacritics_map[char][predictions[i]])\n",
    "        else:\n",
    "            # If no transformation needed, or character not in map, add original character\n",
    "            result.append(char)\n",
    "    return ''.join(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_text = \"Nasilsiniz acaba, her sey yolunda mi?\"\n",
    "\n",
    "# Get the prediction\n",
    "predictions = predict(model, tokenizer, original_text, device)\n",
    "\n",
    "decoded_prediction = apply_diacritics(original_text, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Nasilsiniz acaba, her sey yolunda mi?\n",
      "Predicted: Nasilsiniz acaba, her sey yolunda mi?\n"
     ]
    }
   ],
   "source": [
    "# Print the original and the predicted output\n",
    "print(\"Original:\", original_text)\n",
    "print(\"Predicted:\", decoded_prediction)  # You may need to map these predictions back to actual diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I -> İ durumu eklenecek"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
