{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1439227,"sourceType":"datasetVersion","datasetId":843370},{"sourceId":8108889,"sourceType":"datasetVersion","datasetId":4789828},{"sourceId":8283698,"sourceType":"datasetVersion","datasetId":4789802}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Import libraries","metadata":{}},{"cell_type":"code","source":"pip install pytorch-crf","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:04:40.560023Z","iopub.execute_input":"2024-05-04T00:04:40.560849Z","iopub.status.idle":"2024-05-04T00:04:53.887313Z","shell.execute_reply.started":"2024-05-04T00:04:40.560804Z","shell.execute_reply":"2024-05-04T00:04:53.886068Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Collecting pytorch-crf\n  Downloading pytorch_crf-0.7.2-py3-none-any.whl.metadata (2.4 kB)\nDownloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\nInstalling collected packages: pytorch-crf\nSuccessfully installed pytorch-crf-0.7.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"pip install unidecode","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:04:26.261287Z","iopub.execute_input":"2024-05-04T00:04:26.261693Z","iopub.status.idle":"2024-05-04T00:04:40.556446Z","shell.execute_reply.started":"2024-05-04T00:04:26.261663Z","shell.execute_reply":"2024-05-04T00:04:40.555112Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting unidecode\n  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\nDownloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hInstalling collected packages: unidecode\nSuccessfully installed unidecode-1.3.8\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport string\nfrom unidecode import unidecode\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\nfrom transformers import AdamW, BertModel, AutoConfig, AutoTokenizer\nfrom torch.optim import Adam\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, get_linear_schedule_with_warmup\nimport time;\nimport datetime\nfrom sklearn.model_selection import train_test_split\nfrom torchcrf import CRF","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:04:53.889117Z","iopub.execute_input":"2024-05-04T00:04:53.889548Z","iopub.status.idle":"2024-05-04T00:05:00.861267Z","shell.execute_reply.started":"2024-05-04T00:04:53.889510Z","shell.execute_reply":"2024-05-04T00:05:00.860196Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():  \n    device = torch.device(\"cuda\")\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n    \nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-05-03T22:56:27.784416Z","iopub.execute_input":"2024-05-03T22:56:27.784774Z","iopub.status.idle":"2024-05-03T22:56:27.791275Z","shell.execute_reply.started":"2024-05-03T22:56:27.784743Z","shell.execute_reply":"2024-05-03T22:56:27.789857Z"},"trusted":true},"execution_count":202,"outputs":[{"name":"stdout","text":"We will use the GPU: Tesla T4\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Read data","metadata":{}},{"cell_type":"code","source":"nlp_train=pd.read_csv(\"/kaggle/input/nlp-dataset/train.csv\", index_col=[0])\nnlp_test=pd.read_csv(\"/kaggle/input/nlp-dataset/test.csv\",index_col=[0],encoding=\"windows-1252\") \n\n#nlp_train=pd.read_csv(\"/kaggle/input/nlp-project-train/train.csv\", index_col=[0])\n#nlp_test=pd.read_csv(\"/kaggle/input/nlp-project-train/test.csv\",index_col=[0],encoding=\"windows-1252\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:05:00.863427Z","iopub.execute_input":"2024-05-04T00:05:00.863919Z","iopub.status.idle":"2024-05-04T00:05:01.110787Z","shell.execute_reply.started":"2024-05-04T00:05:00.863891Z","shell.execute_reply":"2024-05-04T00:05:01.109756Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Functions to manipulate data","metadata":{}},{"cell_type":"code","source":"def convert_to_ascii(text):\n    text = unidecode(text)\n    text = text.replace(\"I\", \"ı\").lower()\n    text = text.replace(\"i\", \"ı\")\n    return text\n\ndef remove_punctuations(text):\n    for punctuation in string.punctuation:\n        text = text.replace(punctuation, '')\n        text = text.replace('  ', ' ')\n    return text.strip()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:05:50.184047Z","iopub.execute_input":"2024-05-04T00:05:50.184983Z","iopub.status.idle":"2024-05-04T00:05:50.191442Z","shell.execute_reply.started":"2024-05-04T00:05:50.184942Z","shell.execute_reply":"2024-05-04T00:05:50.190550Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def pad_sequence(sequence, max_length, padding_value=0):\n    sequence = (sequence + [padding_value] * (max_length - len(sequence)))\n    return sequence\n\ndef pad_sentences(encoded_sentences, max_sentence_length, max_word_length):\n    padded_sentences = []\n    for sentence in encoded_sentences:\n        padded_words = [pad_sequence(word, max_word_length) for word in sentence]\n        while len(padded_words) < max_sentence_length:\n            padded_words.append([0] * max_word_length)\n        padded_sentences.append(padded_words)\n    return padded_sentences","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:05:50.473642Z","iopub.execute_input":"2024-05-04T00:05:50.474002Z","iopub.status.idle":"2024-05-04T00:05:50.480616Z","shell.execute_reply.started":"2024-05-04T00:05:50.473973Z","shell.execute_reply":"2024-05-04T00:05:50.479595Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def encode_character(char):\n    diacritic_chars = \"iöü\"\n    return 2 if char in diacritic_chars else 1\n\ndef encode_word(word):\n    return [encode_character(char) for char in word]\n\ndef encode_sentences(text, max_word_length, max_sentence_length):\n    sentences = text\n    encoded_sentences = [[encode_word(word) for word in sentence.split()] for sentence in sentences]\n    padded_encoded_sentences = pad_sentences(encoded_sentences, max_sentence_length, max_word_length)\n    return padded_encoded_sentences\n\n# Example usage\n#text = [\"Bu öykü\", \"Bu, Mustafa'nın hikayesi\"]\n#encoded_output = encode_sentences(text)\n#print(encoded_output[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:05:50.800738Z","iopub.execute_input":"2024-05-04T00:05:50.801648Z","iopub.status.idle":"2024-05-04T00:05:50.807558Z","shell.execute_reply.started":"2024-05-04T00:05:50.801614Z","shell.execute_reply":"2024-05-04T00:05:50.806671Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"##\n#  0 for letters which always be used with base format (we will encode as index)\n#  1 for letters which is used as non-diacritic\n#  2 for letters whic is used as diacritic\n#(we will encode as index)\n\n## example for word öykü -->  4x3 matrix,\n# [0 0 1]\n# [1 0 0]\n# [1 0 0]\n# [0 0 1]\n\n## example for word mustafa\n# [1 0 0]\n# [0 1 0]\n# [1 0 0]\n# [1 0 0]\n# [1 0 0]\n# [1 0 0]\n# [1 0 0]","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:05:51.811476Z","iopub.execute_input":"2024-05-04T00:05:51.812134Z","iopub.status.idle":"2024-05-04T00:05:51.816506Z","shell.execute_reply.started":"2024-05-04T00:05:51.812099Z","shell.execute_reply":"2024-05-04T00:05:51.815455Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"nlp_train['Sentence'] = nlp_train['Sentence'].apply(remove_punctuations)\nnlp_train[\"Label\"] = nlp_train[\"Sentence\"]\nnlp_train[\"Sentence\"] = nlp_train[\"Sentence\"].apply(convert_to_ascii)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:05:52.813737Z","iopub.execute_input":"2024-05-04T00:05:52.814132Z","iopub.status.idle":"2024-05-04T00:05:56.794257Z","shell.execute_reply.started":"2024-05-04T00:05:52.814085Z","shell.execute_reply":"2024-05-04T00:05:56.793403Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"nlp_train[\"Label\"][0:2]","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:05:56.796084Z","iopub.execute_input":"2024-05-04T00:05:56.796694Z","iopub.status.idle":"2024-05-04T00:05:56.805320Z","shell.execute_reply.started":"2024-05-04T00:05:56.796659Z","shell.execute_reply":"2024-05-04T00:05:56.804345Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"ID\n0    sınıf havuz ve açık deniz çalışmalarıyla tüm d...\n1    bu standart sualtında kendini rahat hisseden h...\nName: Label, dtype: object"},"metadata":{}}]},{"cell_type":"code","source":"sentences_train, labels_train = nlp_train.Sentence.values, nlp_train.Label.values","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:05:56.806505Z","iopub.execute_input":"2024-05-04T00:05:56.806810Z","iopub.status.idle":"2024-05-04T00:05:56.813734Z","shell.execute_reply.started":"2024-05-04T00:05:56.806786Z","shell.execute_reply":"2024-05-04T00:05:56.812829Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"Tokenizer","metadata":{}},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:05:56.815988Z","iopub.execute_input":"2024-05-04T00:05:56.816296Z","iopub.status.idle":"2024-05-04T00:05:57.536961Z","shell.execute_reply.started":"2024-05-04T00:05:56.816270Z","shell.execute_reply":"2024-05-04T00:05:57.536184Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/60.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7020c09f0e7e4154b5ce85ea11d091e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd115dedf619497da0922320d56633fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/251k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06ae11c5d5c9444ea1abc89cc2256535"}},"metadata":{}}]},{"cell_type":"markdown","source":"Functions to segment data","metadata":{}},{"cell_type":"code","source":"def segment_text(sentence, label, max_length=128, max_length_word=70, overlap=50):\n    tokens = tokenizer.tokenize(sentence)\n\n    # Check if the length of any word exceeds the maximum length allowed for a word\n    if any(len(word) > max_length_word for word in sentence.split()):\n        return None, None\n\n    # Check if the total number of tokens exceeds the maximum length\n    if len(tokens) > max_length:\n        return None, None\n\n    # If all checks pass, return the original sentence and label\n    return sentence, label\n    \n    \ndef data_segments(sentences, labels, max_length=32):\n    all_text = []\n    all_labels = []\n\n    for sentence, label in zip(sentences, labels):\n        segment_s, segment_l = segment_text(sentence, label, max_length=max_length, overlap=50)\n        if segment_s:\n            all_text.append(segment_s)\n            all_labels.append(segment_l)\n            \n    return all_text, all_labels","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:05:57.538230Z","iopub.execute_input":"2024-05-04T00:05:57.538900Z","iopub.status.idle":"2024-05-04T00:05:57.546712Z","shell.execute_reply.started":"2024-05-04T00:05:57.538864Z","shell.execute_reply":"2024-05-04T00:05:57.545792Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_sentences_segment, train_labels_segment = data_segments(sentences_train, labels_train)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:05:57.547972Z","iopub.execute_input":"2024-05-04T00:05:57.548427Z","iopub.status.idle":"2024-05-04T00:06:04.824546Z","shell.execute_reply.started":"2024-05-04T00:05:57.548395Z","shell.execute_reply":"2024-05-04T00:06:04.823509Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (7014 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer.tokenize(train_sentences_segment[25748])","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:06:04.827232Z","iopub.execute_input":"2024-05-04T00:06:04.827964Z","iopub.status.idle":"2024-05-04T00:06:04.834585Z","shell.execute_reply.started":"2024-05-04T00:06:04.827929Z","shell.execute_reply":"2024-05-04T00:06:04.833483Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"['sıf', '##ren', '##ız']"},"metadata":{}}]},{"cell_type":"code","source":"labels_train_encoded = encode_sentences(train_labels_segment, max_word_length=70, max_sentence_length=40)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:06:04.835913Z","iopub.execute_input":"2024-05-04T00:06:04.836305Z","iopub.status.idle":"2024-05-04T00:06:10.267926Z","shell.execute_reply.started":"2024-05-04T00:06:04.836271Z","shell.execute_reply":"2024-05-04T00:06:10.266815Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"np.array(labels_train_encoded).shape","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:06:10.269206Z","iopub.execute_input":"2024-05-04T00:06:10.269509Z","iopub.status.idle":"2024-05-04T00:06:20.689010Z","shell.execute_reply.started":"2024-05-04T00:06:10.269483Z","shell.execute_reply":"2024-05-04T00:06:20.687945Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(37002, 40, 70)"},"metadata":{}}]},{"cell_type":"code","source":"indices=tokenizer.batch_encode_plus(train_sentences_segment,max_length=32,add_special_tokens=True, return_attention_mask=True,padding=True,truncation=True)\ninput_ids=indices[\"input_ids\"]\nattention_masks=indices[\"attention_mask\"]\nprint(input_ids[0])\nprint(train_sentences_segment[0])\nprint(attention_masks[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:06:20.690572Z","iopub.execute_input":"2024-05-04T00:06:20.690965Z","iopub.status.idle":"2024-05-04T00:06:22.908480Z","shell.execute_reply.started":"2024-05-04T00:06:20.690932Z","shell.execute_reply":"2024-05-04T00:06:22.907334Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"[2, 3825, 8725, 1992, 24045, 2350, 2028, 15007, 2116, 3970, 2218, 13526, 1027, 26905, 24419, 3575, 1028, 1991, 21070, 2187, 16090, 5538, 14330, 2033, 2002, 9474, 2293, 3, 0, 0, 0, 0]\nsınıf havuz ve acık denız calısmalarıyla tum dunyada gecerlı basarılı bır standart olusturmustur\n[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"def align_labels_with_tokens(labels_list, tokenized_input):\n    aligned_labels = []\n    for labels, input_ids in zip(labels_list, tokenized_input):\n        word_ids = tokenizer.convert_ids_to_tokens(input_ids)  # Get the token strings\n        aligned_label = []\n        current_word = 0\n        for word_piece, label in zip(word_ids, labels):\n            if tokenizer.convert_tokens_to_ids(word_piece) == tokenizer.cls_token_id or \\\n               tokenizer.convert_tokens_to_ids(word_piece) == tokenizer.sep_token_id:\n                # Special tokens have a label \"ignore\" or some specific label\n                aligned_label.append([-100] * 70)  # Typically ignored in loss computation\n            else:\n                if word_piece.startswith(\"##\"):\n                    # Continuation of the previous word\n                    aligned_label.append(labels[current_word])\n                else:\n                    # A new word\n                    current_word += 1\n                    aligned_label.append(labels[current_word])\n        aligned_labels.append(aligned_label)\n    return aligned_labels\n\n# Adjust the `train_labels` structure to fit the exact format needed\naligned_train_labels = align_labels_with_tokens(labels_train_encoded, input_ids)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:06:22.909623Z","iopub.execute_input":"2024-05-04T00:06:22.909905Z","iopub.status.idle":"2024-05-04T00:06:30.858637Z","shell.execute_reply.started":"2024-05-04T00:06:22.909880Z","shell.execute_reply":"2024-05-04T00:06:30.857585Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"np.array(input_ids).shape","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:13:03.120802Z","iopub.execute_input":"2024-05-04T00:13:03.121195Z","iopub.status.idle":"2024-05-04T00:13:03.257782Z","shell.execute_reply.started":"2024-05-04T00:13:03.121164Z","shell.execute_reply":"2024-05-04T00:13:03.256627Z"},"trusted":true},"execution_count":26,"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"(37002, 32)"},"metadata":{}}]},{"cell_type":"code","source":"np.array(aligned_train_labels).shape","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:08:14.822703Z","iopub.execute_input":"2024-05-04T00:08:14.823116Z","iopub.status.idle":"2024-05-04T00:08:23.135822Z","shell.execute_reply.started":"2024-05-04T00:08:14.823083Z","shell.execute_reply":"2024-05-04T00:08:23.134700Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"(37002, 32, 70)"},"metadata":{}}]},{"cell_type":"code","source":"\"\"\"\nindices=tokenizer.batch_encode_plus(train_labels_segment,max_length=32,add_special_tokens=True, return_attention_mask=True,padding=True,truncation=True)\noutput_ids=indices[\"input_ids\"]\nprint(output_ids[0])\nprint(train_labels_segment[0])\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-05-03T22:58:14.397728Z","iopub.execute_input":"2024-05-03T22:58:14.398104Z","iopub.status.idle":"2024-05-03T22:58:16.059560Z","shell.execute_reply.started":"2024-05-03T22:58:14.398058Z","shell.execute_reply":"2024-05-03T22:58:16.058604Z"},"trusted":true},"execution_count":228,"outputs":[{"name":"stdout","text":"[2, 3825, 8725, 1992, 2416, 4456, 24513, 2525, 5292, 5953, 4165, 1996, 5538, 27202, 2293, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\nsınıf havuz ve açık deniz çalışmalarıyla tüm dünyada geçerli başarılı bir standart oluşturmuştur\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Prepare train and test data","metadata":{}},{"cell_type":"code","source":"# Use 99% for training and 1% for validation.\ntrain_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids,aligned_train_labels, \n                                                            random_state=42, test_size=0.2)\n# Do the same for the masks.\ntrain_masks, validation_masks, _, _ = train_test_split(attention_masks, aligned_train_labels,\n                                             random_state=42, test_size=0.2)\n\n# Convert all of our data into torch tensors, the required datatype for our model\ntrain_inputs = torch.tensor(train_inputs)\nvalidation_inputs = torch.tensor(validation_inputs)\ntrain_labels = torch.tensor(train_labels, dtype=torch.long)\nvalidation_labels = torch.tensor(validation_labels, dtype=torch.long)\ntrain_masks = torch.tensor(train_masks, dtype=torch.long)\nvalidation_masks = torch.tensor(validation_masks, dtype=torch.long)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:13:31.503502Z","iopub.execute_input":"2024-05-04T00:13:31.504619Z","iopub.status.idle":"2024-05-04T00:13:44.710507Z","shell.execute_reply.started":"2024-05-04T00:13:31.504583Z","shell.execute_reply":"2024-05-04T00:13:44.709200Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def format_time(elapsed):\n    # Round to the nearest second.\n    elapsed_rounded = int(round((elapsed)))\n    return str(datetime.timedelta(seconds=elapsed_rounded))\n\n# Function to calculate the accuracy of our predictions vs labels\ndef flat_accuracy(preds, labels):\n    #print(\"preds\", preds.shape)\n    #print(\"labels\", labels.shape)\n    pred_flat = np.argmax(preds, axis=2).flatten()\n    print(\"argmax\", pred_flat[0])\n    labels_flat = labels.flatten()\n    return np.sum(pred_flat == labels_flat) / len(labels_flat)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T17:20:44.342379Z","iopub.execute_input":"2024-05-03T17:20:44.343132Z","iopub.status.idle":"2024-05-03T17:20:44.349170Z","shell.execute_reply.started":"2024-05-03T17:20:44.343102Z","shell.execute_reply":"2024-05-03T17:20:44.348079Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":"## Combined Model","metadata":{}},{"cell_type":"code","source":"# unique_labels = nlp_train['Label'].unique()\n# num_labels = len(unique_labels)\n\n# tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n# encoded_batch = tokenizer(train_sentences_segment, padding=True, truncation=True, return_tensors=\"pt\")\n# input_ids = encoded_batch['input_ids']\n# attention_mask_train = encoded_batch['attention_mask']\n\n# train_data = TensorDataset(train_inputs, train_masks, train_labels)\n# train_sampler = RandomSampler(train_data)\n# train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=32)","metadata":{"execution":{"iopub.status.busy":"2024-05-03T17:20:44.831767Z","iopub.execute_input":"2024-05-03T17:20:44.832102Z","iopub.status.idle":"2024-05-03T17:20:44.836422Z","shell.execute_reply.started":"2024-05-03T17:20:44.832077Z","shell.execute_reply":"2024-05-03T17:20:44.835547Z"},"trusted":true},"execution_count":53,"outputs":[]},{"cell_type":"code","source":"# class BertBiLSTMCRF(nn.Module):\n#     def __init__(self, bert_model, num_labels, lstm_hidden_dim):\n#         super(BertBiLSTMCRF, self).__init__()\n#         self.bert = BertModel.from_pretrained(bert_model)\n#         self.lstm = nn.LSTM(input_size=self.bert.config.hidden_size,\n#                             hidden_size=lstm_hidden_dim,\n#                             num_layers=1,\n#                             bidirectional=True,\n#                             batch_first=True)\n#         self.dropout = nn.Dropout(0.1)\n#         self.classifier = nn.Linear(lstm_hidden_dim * 2, num_labels)\n#         self.crf = CRF(num_labels, batch_first=True)\n\n#     def forward(self, input_ids, attention_mask=None, labels=None):\n#         outputs = self.bert(input_ids, attention_mask=attention_mask)\n#         sequence_output = outputs[0]\n#         lstm_output, _ = self.lstm(sequence_output)\n#         lstm_output = self.dropout(lstm_output)\n#         logits = self.classifier(lstm_output)\n#         if labels is not None:\n#             loss = -self.crf(logits, labels, mask=attention_mask.byte())\n#             return loss, logits\n#         else:\n#             return logits\n\n#     def predict(self, input_ids, attention_mask=None):\n#         with torch.no_grad():\n#             logits = self.forward(input_ids, attention_mask)\n#             return self.crf.decode(logits, mask=attention_mask.byte())\n        \n\"\"\"        \nclass BertBiLSTMCRF(nn.Module):\n    def __init__(self, bert_model, num_labels, lstm_hidden_dim, lstm_layers=1):\n        super(BertBiLSTMCRF, self).__init__()\n        self.bert = BertModel.from_pretrained(bert_model)\n        self.lstm = nn.LSTM(input_size=self.bert.config.hidden_size,\n                            hidden_size=lstm_hidden_dim,\n                            num_layers=lstm_layers,\n                            bidirectional=True,\n                            batch_first=True)\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(lstm_hidden_dim * 2, num_labels)\n        self.crf = CRF(num_labels, batch_first=True)\n\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\n        sequence_output = outputs[0]\n        lstm_output, _ = self.lstm(sequence_output)\n        lstm_output = self.dropout(lstm_output)\n        logits = self.classifier(sequence_output)\n        if labels is not None:\n            loss = -self.crf(logits, labels, mask=attention_mask.byte())  # Calculate loss using CRF\n            return loss, logits\n        else:\n            return logits\n\n    def predict(self, input_ids, attention_mask=None):\n        with torch.no_grad():\n            logits = self.forward(input_ids, attention_mask)\n            return self.crf.decode(logits, mask=attention_mask.byte())\n            \n\"\"\"\n\"\"\"\nclass BertCRF(nn.Module):\n    def __init__(self, bert_model, num_labels):\n        super(BertCRF, self).__init__()\n        self.bert = BertModel.from_pretrained(bert_model)\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n        self.crf = CRF(num_labels, batch_first=True)\n\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\n        sequence_output = outputs[0]\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n        if labels is not None:\n            loss = -self.crf(logits, labels, mask=attention_mask.byte())  # Calculate loss using CRF\n            return loss, logits\n        else:\n            return logits\n\n    def predict(self, input_ids, attention_mask=None):\n        with torch.no_grad():\n            logits = self.forward(input_ids, attention_mask)\n            return self.crf.decode(logits, mask=attention_mask.byte())\n\"\"\"\n\"\"\"\nclass BertCRF(nn.Module):\n    def __init__(self, bert_model, num_labels):\n        super(BertCRF, self).__init__()\n        self.bert = BertModel.from_pretrained(bert_model)\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(self.bert.config.hidden_size, num_labels)\n\n    def forward(self, input_ids, attention_mask=None, labels=None):\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\n        sequence_output = outputs[0]\n        sequence_output = self.dropout(sequence_output)\n        logits = self.classifier(sequence_output)\n        loss = 0.0\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            # Only keep active parts of the loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.classifier.out_features)[active_loss]\n                active_labels = labels.view(-1)[active_loss]\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.classifier.out_features), labels.view(-1))\n            return loss, logits\n        else:\n            return loss, logits\n\n    def predict(self, input_ids, attention_mask=None):\n        logits = self.forward(input_ids, attention_mask)\n        return torch.argmax(logits, dim=-1)\n\"\"\"\n\nimport torch.nn.functional as F\n\nclass DiacritizationModel(nn.Module):\n    def __init__(self, bert_model, hidden_dim, num_words=32, num_chars=70, num_classes=3):\n        super(DiacritizationModel, self).__init__()\n        self.bert = BertModel.from_pretrained(bert_model)\n        self.gru = nn.GRU(self.bert.config.hidden_size, hidden_dim, batch_first=True)\n        # Assuming each token output from GRU should decide the labels of multiple characters\n        self.classifier = nn.Linear(hidden_dim, num_chars * num_classes)\n        self.num_words = num_words\n        self.num_chars = num_chars\n        self.num_classes = num_classes\n\n    def forward(self, input_ids, attention_mask):\n        # BERT outputs\n        outputs = self.bert(input_ids, attention_mask=attention_mask)\n        sequence_output = outputs.last_hidden_state  # (batch_size, seq_length, hidden_size)\n\n        # GRU processing\n        gru_output, _ = self.gru(sequence_output)  # (batch_size, seq_length, hidden_dim)\n\n        # Classification to character-level diacritic status\n        prediction = self.classifier(gru_output)  # (batch_size, seq_length, num_chars * num_classes)\n        # Reshape to make each position in the sequence have a (num_chars, num_classes) prediction\n        prediction = prediction.view(-1, self.num_words, self.num_chars, self.num_classes)  # Reshaping to (batch_size, 32, 70, 3)\n\n        return prediction\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:56:48.963766Z","iopub.execute_input":"2024-05-04T00:56:48.964183Z","iopub.status.idle":"2024-05-04T00:56:48.979977Z","shell.execute_reply.started":"2024-05-04T00:56:48.964150Z","shell.execute_reply":"2024-05-04T00:56:48.979025Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"\"\"\"\ndef train_model(model, train_dataloader, validation_dataloader, device, epochs=4, gradient_accumulation_steps=1):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.95)\n    criterion = nn.CrossEntropyLoss() \n    \n    for epoch_i in range(epochs):\n        print(f'======== Epoch {epoch_i + 1} / {epochs} ========')\n        total_train_loss = 0\n        \n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            batch = tuple(t.to(device) for t in batch)\n            b_input_ids, b_input_mask, b_labels = batch\n            \n            model.zero_grad()        \n            loss, logits = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n            #loss = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n            \n            loss = loss / gradient_accumulation_steps\n            loss.backward()\n            \n            total_train_loss += loss.item()\n            if (step + 1) % gradient_accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n            \n            if step % 40 == 0 and not step == 0:\n                print(f'  Batch {step:>5,} of {len(train_dataloader):>5,}. Loss: {loss.item():.2f}')\n        \n        avg_train_loss = total_train_loss / len(train_dataloader)\n        print(f\"  Average training loss: {avg_train_loss:.2f}\")\n\n        print(\"\\nRunning Validation...\")\n        total_eval_accuracy = 0\n        total_eval_loss = 0\n        model.eval()\n\n        for batch in validation_dataloader:\n            batch = tuple(t.to(device) for t in batch)\n            b_input_ids, b_input_mask, b_labels = batch\n            \n            with torch.no_grad():\n                loss, logits = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n                #loss = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\n            total_eval_loss += loss.item()\n            \n            logits = logits.detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n            #print(\"logits\", logits.shape)\n            #print(\"labels,\", label_ids.shape)\n            total_eval_accuracy += flat_accuracy(logits, label_ids)\n        \n        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n        print(f\"  Validation accuracy: {avg_val_accuracy:.2f}\")\n        avg_val_loss = total_eval_loss / len(validation_dataloader)\n        print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n\n    print(\"done\")\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:56:49.746027Z","iopub.execute_input":"2024-05-04T00:56:49.747046Z","iopub.status.idle":"2024-05-04T00:56:49.756110Z","shell.execute_reply.started":"2024-05-04T00:56:49.747003Z","shell.execute_reply":"2024-05-04T00:56:49.755061Z"},"trusted":true},"execution_count":49,"outputs":[{"execution_count":49,"output_type":"execute_result","data":{"text/plain":"'\\ndef train_model(model, train_dataloader, validation_dataloader, device, epochs=4, gradient_accumulation_steps=1):\\n    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.95)\\n    criterion = nn.CrossEntropyLoss() \\n    \\n    for epoch_i in range(epochs):\\n        print(f\\'======== Epoch {epoch_i + 1} / {epochs} ========\\')\\n        total_train_loss = 0\\n        \\n        model.train()\\n        for step, batch in enumerate(train_dataloader):\\n            batch = tuple(t.to(device) for t in batch)\\n            b_input_ids, b_input_mask, b_labels = batch\\n            \\n            model.zero_grad()        \\n            loss, logits = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\\n            #loss = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\\n            \\n            loss = loss / gradient_accumulation_steps\\n            loss.backward()\\n            \\n            total_train_loss += loss.item()\\n            if (step + 1) % gradient_accumulation_steps == 0:\\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\\n                optimizer.step()\\n                scheduler.step()\\n                model.zero_grad()\\n            \\n            if step % 40 == 0 and not step == 0:\\n                print(f\\'  Batch {step:>5,} of {len(train_dataloader):>5,}. Loss: {loss.item():.2f}\\')\\n        \\n        avg_train_loss = total_train_loss / len(train_dataloader)\\n        print(f\"  Average training loss: {avg_train_loss:.2f}\")\\n\\n        print(\"\\nRunning Validation...\")\\n        total_eval_accuracy = 0\\n        total_eval_loss = 0\\n        model.eval()\\n\\n        for batch in validation_dataloader:\\n            batch = tuple(t.to(device) for t in batch)\\n            b_input_ids, b_input_mask, b_labels = batch\\n            \\n            with torch.no_grad():\\n                loss, logits = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\\n                #loss = model(b_input_ids, attention_mask=b_input_mask, labels=b_labels)\\n            total_eval_loss += loss.item()\\n            \\n            logits = logits.detach().cpu().numpy()\\n            label_ids = b_labels.to(\\'cpu\\').numpy()\\n            #print(\"logits\", logits.shape)\\n            #print(\"labels,\", label_ids.shape)\\n            total_eval_accuracy += flat_accuracy(logits, label_ids)\\n        \\n        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\\n        print(f\"  Validation accuracy: {avg_val_accuracy:.2f}\")\\n        avg_val_loss = total_eval_loss / len(validation_dataloader)\\n        print(f\"  Validation Loss: {avg_val_loss:.2f}\")\\n\\n    print(\"done\")\\n'"},"metadata":{}}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\ndef train_model(model, train_dataloader, validation_dataloader, device, epochs=4, gradient_accumulation_steps=1):\n    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.95)\n    criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignoring padding index for loss calculation\n\n    for epoch_i in range(epochs):\n        print(f'======== Epoch {epoch_i + 1} / {epochs} ========')\n        total_train_loss = 0\n        \n        model.train()\n        for step, batch in enumerate(train_dataloader):\n            b_input_ids, b_input_mask, b_labels = [t.to(device) for t in batch]  # Assuming batch is already a tuple\n            \n            model.zero_grad()\n            outputs = model(b_input_ids, attention_mask=b_input_mask)  # model should only return the logits now\n            # Assuming labels are already properly shaped (batch_size, 32, 70)\n            loss = criterion(outputs.view(-1, 3), b_labels.view(-1))  # Reshape for CrossEntropyLoss\n            \n            loss = loss / gradient_accumulation_steps\n            loss.backward()\n            \n            total_train_loss += loss.item()\n            if (step + 1) % gradient_accumulation_steps == 0:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n                optimizer.step()\n                scheduler.step()\n                model.zero_grad()\n            \n            if step % 40 == 0 and not step == 0:\n                print(f'  Batch {step:>5,} of {len(train_dataloader):>5,}. Loss: {loss.item():.2f}')\n        \n        avg_train_loss = total_train_loss / len(train_dataloader)\n        print(f\"  Average training loss: {avg_train_loss:.2f}\")\n\n        print(\"\\nRunning Validation...\")\n        total_eval_loss = 0\n        model.eval()\n\n        for batch in validation_dataloader:\n            b_input_ids, b_input_mask, b_labels = [t.to(device) for t in batch]\n            \n            with torch.no_grad():\n                outputs = model(b_input_ids, attention_mask=b_input_mask)\n                loss = criterion(outputs.view(-1, 3), b_labels.view(-1))\n                total_eval_loss += loss.item()\n        \n        avg_val_loss = total_eval_loss / len(validation_dataloader)\n        print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n\n    print(\"Training complete.\")\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:56:50.451911Z","iopub.execute_input":"2024-05-04T00:56:50.452427Z","iopub.status.idle":"2024-05-04T00:56:50.467134Z","shell.execute_reply.started":"2024-05-04T00:56:50.452395Z","shell.execute_reply":"2024-05-04T00:56:50.466029Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"num_labels = len(set(train_sentences_segment))\n# num_labels = len(unique_labels)\n# num_labels=5\nlstm_hidden_dim = 64\nbert_model = 'dbmdz/bert-base-turkish-cased'\nepochs = 5\nbatch_size = 16\ngradient_accumulation_steps = 5\n\n# model = BertBiLSTMCRF(bert_model, num_labels, lstm_hidden_dim)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ntorch.cuda.empty_cache()\n\n#model = BertBiLSTMCRF(bert_model, num_labels, lstm_hidden_dim)\n#model = BertCRF(bert_model, num_labels)\nmodel = DiacritizationModel(bert_model, hidden_dim=256)\n\n#model.bert = model.bert.to(device)\n# Do some operations if necessary\n#model.lstm = model.lstm.to(device)\n#model.classifier = model.classifier.to(device)\n#model.crf = model.crf.to(device)\n\nmodel.to(device)\n\n\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, pin_memory=True)\nvalidation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\nvalidation_sampler = SequentialSampler(validation_data)\nvalidation_loader = DataLoader(validation_data, batch_size=batch_size, shuffle=False, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:56:50.879615Z","iopub.execute_input":"2024-05-04T00:56:50.879971Z","iopub.status.idle":"2024-05-04T00:56:51.695882Z","shell.execute_reply.started":"2024-05-04T00:56:50.879945Z","shell.execute_reply":"2024-05-04T00:56:51.694968Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"train_model(model, train_loader, validation_loader, device, epochs, gradient_accumulation_steps)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T00:56:51.697487Z","iopub.execute_input":"2024-05-04T00:56:51.697775Z","iopub.status.idle":"2024-05-04T00:56:52.028235Z","shell.execute_reply.started":"2024-05-04T00:56:51.697750Z","shell.execute_reply":"2024-05-04T00:56:52.026814Z"},"trusted":true},"execution_count":52,"outputs":[{"name":"stdout","text":"======== Epoch 1 / 5 ========\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [4,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [5,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [6,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [7,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [8,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [9,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [10,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [11,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [12,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [13,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [14,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [15,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [16,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [17,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [18,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [19,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [20,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [21,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [22,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [23,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [24,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [25,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [26,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [27,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [28,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [29,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [30,0,0] Assertion `t >= 0 && t < n_classes` failed.\n/usr/local/src/pytorch/aten/src/ATen/native/cuda/Loss.cu:250: nll_loss_forward_reduce_cuda_kernel_2d: block: [0,0,0], thread: [31,0,0] Assertion `t >= 0 && t < n_classes` failed.\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient_accumulation_steps\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[50], line 23\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_dataloader, validation_dataloader, device, epochs, gradient_accumulation_steps)\u001b[0m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m), b_labels\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))  \u001b[38;5;66;03m# Reshape for CrossEntropyLoss\u001b[39;00m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m gradient_accumulation_steps\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m total_train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m gradient_accumulation_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"],"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}