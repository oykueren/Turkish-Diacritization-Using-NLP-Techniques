{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 577,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n",
    "from transformers import AdamW, BertModel, AutoConfig, AutoTokenizer\n",
    "from torch.optim import Adam\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import time;\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchcrf import CRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-crf\n",
    "# !pip install unidecode\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install --upgrade torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    \n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp_train=pd.read_csv(\"/kaggle/input/nlp-project-train/train.csv\", index_col=[0])\n",
    "# nlp_test=pd.read_csv(\"/kaggle/input/nlp-project-train/test.csv\",index_col=[0],encoding=\"windows-1252\") \n",
    "\n",
    "nlp_train=pd.read_csv(\"train.csv\", index_col=[0])\n",
    "nlp_test=pd.read_csv(\"test.csv\",index_col=[0],encoding=\"windows-1252\") \n",
    "# nlp_train = nlp_train[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ascii(sentence):\n",
    "    text = unidecode(sentence)\n",
    "    return text\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "        text = text.replace('  ', ' ')\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sentences(sentences, max_length=100):\n",
    "    punctuations = {'.', ',', ';', ':'}\n",
    "    results = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        while len(sentence) > max_length:\n",
    "            # find last punctuation before max_length\n",
    "            split_pos = -1\n",
    "            for p in punctuations:\n",
    "                pos = sentence.rfind(p, 0, max_length + 1)\n",
    "                if pos > split_pos:\n",
    "                    split_pos = pos\n",
    "            \n",
    "            # If no punctuation found, split at the last space before max_length\n",
    "            if split_pos == -1:\n",
    "                split_pos = sentence.rfind(' ', 0, max_length + 1)\n",
    "            \n",
    "            # If no space found, just split at max_length\n",
    "            if split_pos == -1:\n",
    "                split_pos = max_length\n",
    "            \n",
    "            # Append the split segment to results\n",
    "            results.append(sentence[:split_pos + 1].strip())\n",
    "            # Move the rest of the sentence forward\n",
    "            sentence = sentence[split_pos + 1:].strip()\n",
    "        \n",
    "        # Append the remainder of the sentence if it's not empty\n",
    "        if sentence:\n",
    "            results.append(sentence)\n",
    "    \n",
    "    return np.array(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_word(sentence):\n",
    "    words = sentence.split()\n",
    "    encoded_sentence = []\n",
    "    for word in words:\n",
    "        encoded_word = []\n",
    "        for char in word:\n",
    "            # print(char)\n",
    "            if char in \"ıöüğşç\":\n",
    "                encoded_word.append(2)\n",
    "            else:\n",
    "                encoded_word.append(1)\n",
    "            # print(encoded_word)\n",
    "        encoded_sentence.append(encoded_word)\n",
    "    return encoded_sentence\n",
    "\n",
    "def padding(text, filling_char, max_length):\n",
    "    if type(text) is str:\n",
    "        text = text + filling_char * (max_length - len(text))\n",
    "    elif type(text) is list:\n",
    "        text = text + [filling_char] * (max_length - len(text))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_puncutations_and_numbers(text):\n",
    "    # Iterate over the string and remove char if it is not a character\n",
    "    characters = \"abcçdefgğhıijklmnoöprsştuüvyzABCÇDEFGĞHIİJKLMNOÖPRSŞTUÜVYZxXwWqQ \"\n",
    "    for char in text:\n",
    "        if char not in characters:\n",
    "            text = text.replace(char, \"\")\n",
    "    # Remove multiple spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw sentences\n",
    "raw_sentences = nlp_train[\"Sentence\"].values\n",
    "# Split sentences\n",
    "sentences = split_sentences(raw_sentences, max_length=200)\n",
    "# Remove punctuations\n",
    "# sentences = [remove_punctuations(s).lower() for s in sentences]\n",
    "sentences = [remove_puncutations_and_numbers(i) for i in sentences]\n",
    "nlp_train[\"Label\"] = nlp_train[\"Sentence\"]\n",
    "# Apply convert to ascii to y_train\n",
    "asci_sentences = [convert_to_ascii(s) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sentences to list of characters\n",
    "char_sentences_original = [list(s) for s in sentences]\n",
    "\n",
    "# Apply map_diacritics to the numeric sentences\n",
    "labels = [map_diacritics(sentence.copy()) for sentence in char_sentences_original]\n",
    "\n",
    "char_sentences = [list(s) for s in asci_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [],
   "source": [
    "characters_dictionary = {\n",
    "    \"a\":1,\n",
    "    \"b\":2,\n",
    "    \"c\":3,\n",
    "    \"ç\":4,\n",
    "    \"d\":5,\n",
    "    \"e\":6,\n",
    "    \"f\":7,\n",
    "    \"g\":8,\n",
    "    \"ğ\":9,\n",
    "    \"h\":10,\n",
    "    \"ı\":11,\n",
    "    \"i\":12,\n",
    "    \"j\":13,\n",
    "    \"k\":14,\n",
    "    \"l\":15,\n",
    "    \"m\":16,\n",
    "    \"n\":17,\n",
    "    \"o\":18,\n",
    "    \"ö\":19,\n",
    "    \"p\":20,\n",
    "    \"r\":21,\n",
    "    \"s\":22,\n",
    "    \"ş\":23,\n",
    "    \"t\":24,\n",
    "    \"u\":25,\n",
    "    \"ü\":26,\n",
    "    \"v\":27,\n",
    "    \"y\":28,\n",
    "    \"z\":29,\n",
    "    \"q\": 30,\n",
    "    \"x\":31,\n",
    "    \"w\":32,\n",
    "    \" \":33    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy char_sentences to numeric_sentences\n",
    "numeric_sentences = [sentence.copy() for sentence in char_sentences]\n",
    "# Map each character to an integer in the sentences array to its value in the characters dictionary\n",
    "for sentence_index in range(len(char_sentences)):\n",
    "    for char_index in range(len(char_sentences[sentence_index])):\n",
    "        # print(characters_dictionary[char_sentences[sentence_index][char_index]])\n",
    "        numeric_sentences[sentence_index][char_index] = characters_dictionary[char_sentences[sentence_index][char_index]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply padding to the numeric sentences\n",
    "max_length = 200\n",
    "for sentence_index in range(len(numeric_sentences)):\n",
    "    numeric_sentences[sentence_index] = padding(numeric_sentences[sentence_index], 0, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_diacritics(text):\n",
    "    for char_index in range(len(text)):\n",
    "        # print(text[char_index])\n",
    "        if text[char_index] in \"ıöüğşç\":\n",
    "            text[char_index] = 1\n",
    "        else:\n",
    "            text[char_index] = 0\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply padding to the labels\n",
    "max_length = 200\n",
    "for sentence_index in range(len(labels)):\n",
    "    labels[sentence_index] = padding(labels[sentence_index], 0, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DiacritizationBiLSTMCRF(\n",
       "  (embedding): Embedding(34, 128)\n",
       "  (lstm): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc): Linear(in_features=256, out_features=2, bias=True)\n",
       "  (crf): CRF(num_tags=2)\n",
       ")"
      ]
     },
     "execution_count": 592,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "\n",
    "class DiacritizationBiLSTMCRF(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_labels):\n",
    "        super(DiacritizationBiLSTMCRF, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim // 2, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(hidden_dim, num_labels)\n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, sentences, labels=None):\n",
    "        # Embedding layer\n",
    "        x = self.embedding(sentences)\n",
    "        \n",
    "        # BiLSTM layer\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        emissions = self.fc(x)\n",
    "\n",
    "        # CRF layer\n",
    "        if labels is not None:\n",
    "            # If labels are provided, calculate the loss\n",
    "            loss = -self.crf(emissions, labels)\n",
    "            return loss\n",
    "        else:\n",
    "            # Otherwise, return the best path\n",
    "            prediction = self.crf.decode(emissions)\n",
    "            return prediction\n",
    "\n",
    "# Example of initializing the model\n",
    "vocab_size = len(characters_dictionary) + 1  # Number of unique characters in your character dictionary + 1 for padding\n",
    "embed_dim = 128\n",
    "hidden_dim = 256\n",
    "num_labels = 2  # Diacritic or not\n",
    "model = DiacritizationBiLSTMCRF(vocab_size, embed_dim, hidden_dim, num_labels)\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1021.8725925990514\n",
      "Epoch 2, Loss: 472.85624215262277\n",
      "Epoch 3, Loss: 374.29310738699775\n",
      "Epoch 4, Loss: 307.85654994419644\n",
      "Epoch 5, Loss: 260.21475655691967\n",
      "Epoch 6, Loss: 224.7394304547991\n",
      "Epoch 7, Loss: 201.57762799944197\n",
      "Epoch 8, Loss: 184.77154889787946\n",
      "Epoch 9, Loss: 171.2187970842634\n",
      "Epoch 10, Loss: 158.77383510044643\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'numeric_sentences' and 'labels' are your preprocessed datasets available as lists of lists\n",
    "# Convert them into tensors\n",
    "numeric_sentences_tensor = torch.tensor(numeric_sentences, dtype=torch.long)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader\n",
    "train_dataset = TensorDataset(numeric_sentences_tensor, labels_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "def train_model(model, data_loader, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for sentences, label_seqs in data_loader:\n",
    "            sentences, label_seqs = sentences.to(device), label_seqs.to(device)\n",
    "            \n",
    "            # Forward pass: Compute predicted y by passing x to the model\n",
    "            loss = model(sentences, labels=label_seqs)\n",
    "            \n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}, Loss: {total_loss / len(data_loader)}\")\n",
    "\n",
    "# Start training\n",
    "train_model(model, train_loader, optimizer, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Original: Bu cumlede bazi Turkce karakterler var.\n",
      "Processed: Bu çümlede bazı Türkce karakterler var.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence, characters_dictionary, max_length=200):\n",
    "    # Remove punctuation and numbers\n",
    "    sentence = ''.join([i for i in sentence if not i.isdigit() and i not in string.punctuation])\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    # Convert to numeric form using the character dictionary\n",
    "    numeric_sentence = [characters_dictionary.get(char, 0) for char in sentence]  # default to 0 if char not found\n",
    "    \n",
    "    # Padding\n",
    "    if len(numeric_sentence) > max_length:\n",
    "        numeric_sentence = numeric_sentence[:max_length]\n",
    "    else:\n",
    "        numeric_sentence += [0] * (max_length - len(numeric_sentence))\n",
    "    \n",
    "    return torch.tensor([numeric_sentence], dtype=torch.long).to(device)\n",
    "\n",
    "def predict(model, sentence, characters_dictionary):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        numeric_sentence = preprocess_sentence(sentence, characters_dictionary)\n",
    "        prediction = model(numeric_sentence)\n",
    "        return prediction[0]  # since we have a batch size of 1\n",
    "\n",
    "# Example sentence\n",
    "test_sentence = \"Bu cumlede bazi Turkce karakterler var.\"\n",
    "\n",
    "# Predict using the model\n",
    "predicted_labels = predict(model, test_sentence, characters_dictionary)\n",
    "print(predicted_labels)\n",
    "\n",
    "# Define labels to diacritics mapping\n",
    "def labels_to_diacritics(text_sentence, labels):\n",
    "    diacritics_map_dict = {\"i\": \"ı\", \"o\": \"ö\", \"u\": \"ü\", \"g\": \"ğ\", \"s\": \"ş\", \"c\": \"ç\"}\n",
    "    output_sentence = \"\"\n",
    "    for i in range(len(text_sentence)):\n",
    "        if labels[i] == 1:\n",
    "            if text_sentence[i] in diacritics_map_dict:\n",
    "                output_sentence += diacritics_map_dict[text_sentence[i]]\n",
    "        else:\n",
    "            output_sentence += text_sentence[i]\n",
    "    return output_sentence\n",
    "\n",
    "# Show the results\n",
    "output_sentence = labels_to_diacritics(test_sentence, predicted_labels)\n",
    "print(\"Original:\", test_sentence)\n",
    "print(\"Processed:\", output_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
