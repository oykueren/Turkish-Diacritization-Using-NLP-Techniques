{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1439227,"sourceType":"datasetVersion","datasetId":843370},{"sourceId":8108889,"sourceType":"datasetVersion","datasetId":4789828},{"sourceId":8283698,"sourceType":"datasetVersion","datasetId":4789802}],"dockerImageVersionId":29860,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pytorch-crf\n!pip install unidecode","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:36:59.378640Z","iopub.execute_input":"2024-05-04T17:36:59.379032Z","iopub.status.idle":"2024-05-04T17:37:14.496050Z","shell.execute_reply.started":"2024-05-04T17:36:59.378989Z","shell.execute_reply":"2024-05-04T17:37:14.495082Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: pytorch-crf in /opt/conda/lib/python3.6/site-packages (0.7.2)\nRequirement already satisfied: unidecode in /opt/conda/lib/python3.6/site-packages (1.1.1)\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install --upgrade transformers","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:37:14.498447Z","iopub.execute_input":"2024-05-04T17:37:14.498869Z","iopub.status.idle":"2024-05-04T17:37:22.297015Z","shell.execute_reply.started":"2024-05-04T17:37:14.498806Z","shell.execute_reply":"2024-05-04T17:37:22.295893Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already up-to-date: transformers in /opt/conda/lib/python3.6/site-packages (4.18.0)\nRequirement already satisfied, skipping upgrade: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.12.1)\nRequirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from transformers) (0.7)\nRequirement already satisfied, skipping upgrade: pyyaml>=5.1 in /opt/conda/lib/python3.6/site-packages (from transformers) (5.3)\nRequirement already satisfied, skipping upgrade: requests in /opt/conda/lib/python3.6/site-packages (from transformers) (2.22.0)\nRequirement already satisfied, skipping upgrade: filelock in /opt/conda/lib/python3.6/site-packages (from transformers) (3.0.12)\nRequirement already satisfied, skipping upgrade: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (0.4.0)\nRequirement already satisfied, skipping upgrade: packaging>=20.0 in /opt/conda/lib/python3.6/site-packages (from transformers) (20.1)\nRequirement already satisfied, skipping upgrade: regex!=2019.12.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (2020.2.20)\nRequirement already satisfied, skipping upgrade: tqdm>=4.27 in /opt/conda/lib/python3.6/site-packages (from transformers) (4.42.0)\nRequirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /opt/conda/lib/python3.6/site-packages (from transformers) (1.5.0)\nRequirement already satisfied, skipping upgrade: sacremoses in /opt/conda/lib/python3.6/site-packages (from transformers) (0.0.38)\nRequirement already satisfied, skipping upgrade: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from transformers) (1.18.1)\nRequirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (3.0.4)\nRequirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2.8)\nRequirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (2019.11.28)\nRequirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->transformers) (1.24.3)\nRequirement already satisfied, skipping upgrade: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers) (2.4.6)\nRequirement already satisfied, skipping upgrade: six in /opt/conda/lib/python3.6/site-packages (from packaging>=20.0->transformers) (1.14.0)\nRequirement already satisfied, skipping upgrade: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (2.2.0)\nRequirement already satisfied, skipping upgrade: joblib in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (0.14.1)\nRequirement already satisfied, skipping upgrade: click in /opt/conda/lib/python3.6/site-packages (from sacremoses->transformers) (7.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install --upgrade torch","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:37:22.298783Z","iopub.execute_input":"2024-05-04T17:37:22.299102Z","iopub.status.idle":"2024-05-04T17:37:30.216517Z","shell.execute_reply.started":"2024-05-04T17:37:22.299058Z","shell.execute_reply":"2024-05-04T17:37:30.215289Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already up-to-date: torch in /opt/conda/lib/python3.6/site-packages (1.10.2)\nRequirement already satisfied, skipping upgrade: dataclasses; python_version < \"3.7\" in /opt/conda/lib/python3.6/site-packages (from torch) (0.7)\nRequirement already satisfied, skipping upgrade: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch) (4.1.1)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport string\nfrom unidecode import unidecode\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\nfrom transformers import AdamW, BertModel, AutoConfig, AutoTokenizer\nfrom torch.optim import Adam\nimport torch.nn as nn\nfrom transformers import AutoTokenizer, get_linear_schedule_with_warmup\nimport time;\nimport datetime\nfrom sklearn.model_selection import train_test_split\nfrom torchcrf import CRF","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:37:30.218101Z","iopub.execute_input":"2024-05-04T17:37:30.218479Z","iopub.status.idle":"2024-05-04T17:37:34.050497Z","shell.execute_reply.started":"2024-05-04T17:37:30.218423Z","shell.execute_reply":"2024-05-04T17:37:34.049587Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"if torch.cuda.is_available():  \n    device = torch.device(\"cuda\")\n    print('We will use the GPU:', torch.cuda.get_device_name(0))\n    \nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:37:34.054133Z","iopub.execute_input":"2024-05-04T17:37:34.054527Z","iopub.status.idle":"2024-05-04T17:37:34.090698Z","shell.execute_reply.started":"2024-05-04T17:37:34.054475Z","shell.execute_reply":"2024-05-04T17:37:34.089738Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"We will use the GPU: Tesla P100-PCIE-16GB\n","output_type":"stream"}]},{"cell_type":"code","source":"nlp_train=pd.read_csv(\"/kaggle/input/nlp-project-train/train.csv\", index_col=[0])\nnlp_test=pd.read_csv(\"/kaggle/input/nlp-project-train/test.csv\",index_col=[0],encoding=\"windows-1252\") \n\n# nlp_train=pd.read_csv(\"train.csv\", index_col=[0])\n# nlp_test=pd.read_csv(\"test.csv\",index_col=[0],encoding=\"windows-1252\") ","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:37:34.092957Z","iopub.execute_input":"2024-05-04T17:37:34.093400Z","iopub.status.idle":"2024-05-04T17:37:34.261608Z","shell.execute_reply.started":"2024-05-04T17:37:34.093352Z","shell.execute_reply":"2024-05-04T17:37:34.260719Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def convert_to_ascii(sentence):\n    text = unidecode(sentence)\n    return text\n\ndef remove_punctuations(text):\n    for punctuation in string.punctuation:\n        text = text.replace(punctuation, '')\n        text = text.replace('  ', ' ')\n    return text.strip()\n\ndef split_sentences(sentences, max_length=100):\n    punctuations = {'.', ',', ';', ':'}\n    results = []\n\n    for sentence in sentences:\n        while len(sentence) > max_length:\n            # find last punctuation before max_length\n            split_pos = -1\n            for p in punctuations:\n                pos = sentence.rfind(p, 0, max_length + 1)\n                if pos > split_pos:\n                    split_pos = pos\n            \n            # If no punctuation found, split at the last space before max_length\n            if split_pos == -1:\n                split_pos = sentence.rfind(' ', 0, max_length + 1)\n            \n            # If no space found, just split at max_length\n            if split_pos == -1:\n                split_pos = max_length\n            \n            # Append the split segment to results\n            results.append(sentence[:split_pos + 1].strip())\n            # Move the rest of the sentence forward\n            sentence = sentence[split_pos + 1:].strip()\n        \n        # Append the remainder of the sentence if it's not empty\n        if sentence:\n            results.append(sentence)\n    \n    return np.array(results)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:37:34.262968Z","iopub.execute_input":"2024-05-04T17:37:34.263371Z","iopub.status.idle":"2024-05-04T17:37:34.276802Z","shell.execute_reply.started":"2024-05-04T17:37:34.263316Z","shell.execute_reply":"2024-05-04T17:37:34.275974Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Raw sentences\nraw_sentences = nlp_train[\"Sentence\"].values\n# Split sentences\nsentences = split_sentences(raw_sentences, max_length=200)\n# Remove punctuations\nsentences = [remove_punctuations(s).lower() for s in sentences]\n# Apply convert to ascii to y_train\nasci_sentences = [convert_to_ascii(s) for s in sentences]","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:37:34.278338Z","iopub.execute_input":"2024-05-04T17:37:34.278690Z","iopub.status.idle":"2024-05-04T17:37:38.583075Z","shell.execute_reply.started":"2024-05-04T17:37:34.278638Z","shell.execute_reply":"2024-05-04T17:37:38.582236Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def encode_word(sentence):\n    for char in sentence:\n        # print(char)\n        if char in \"ıöüğşç\":\n            # replace char with 2\n            sentence = sentence.replace(char, '2')\n        else:\n            sentence = sentence.replace(char, '1')\n    return sentence","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:37:38.584410Z","iopub.execute_input":"2024-05-04T17:37:38.584746Z","iopub.status.idle":"2024-05-04T17:37:38.590328Z","shell.execute_reply.started":"2024-05-04T17:37:38.584700Z","shell.execute_reply":"2024-05-04T17:37:38.589449Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"sentences_diacritics = sentences.copy()\nfor sentence_index in range(len(sentences_diacritics)):\n    sentence = sentences_diacritics[sentence_index]\n    new_sentence = encode_word(sentence)\n    sentences_diacritics[sentence_index] = new_sentence","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:37:38.591817Z","iopub.execute_input":"2024-05-04T17:37:38.592269Z","iopub.status.idle":"2024-05-04T17:37:40.561969Z","shell.execute_reply.started":"2024-05-04T17:37:38.592195Z","shell.execute_reply":"2024-05-04T17:37:40.561056Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"sentences_diacritics[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:37:40.563326Z","iopub.execute_input":"2024-05-04T17:37:40.563721Z","iopub.status.idle":"2024-05-04T17:37:40.572203Z","shell.execute_reply.started":"2024-05-04T17:37:40.563670Z","shell.execute_reply":"2024-05-04T17:37:40.571265Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"'121211111111111122111111112112211111211111211121111111121111111211212111111111111111112111112111'"},"metadata":{}}]},{"cell_type":"code","source":"def align_tokens_with_diacritics(texts, diacritics, tokenizer, label_dict):\n    tokenized_texts = tokenizer(texts, padding=True, truncation=True, return_offsets_mapping=True, return_tensors=\"pt\")\n    input_ids = tokenized_texts['input_ids']\n    attention_masks = tokenized_texts['attention_mask']\n    token_type_ids = tokenized_texts['token_type_ids']\n    offset_mappings = tokenized_texts['offset_mapping']\n    \n    # Initialize the labels for each token to a default numeric value, e.g., label_dict['O'] for 'Other'\n    labels = [[label_dict['O']] * len(input_id) for input_id in input_ids]\n\n    for i, offsets in enumerate(offset_mappings):\n        for token_index, (start, end) in enumerate(offsets):\n            if start == end:  # Special tokens, skip them\n                continue\n            # Extract the original character sequence corresponding to the current token\n            char_sequence = texts[i][start:end]\n            # Assuming diacritics are stored in a similar structure as texts\n            corresponding_diacritic = diacritics[i][start:end]\n            # Assign the diacritic label to the first character's diacritic or any suitable logic\n            # Use the dictionary to convert string labels to integers\n            labels[i][token_index] = label_dict[corresponding_diacritic[0]] if corresponding_diacritic else label_dict['O']\n\n    return input_ids, attention_masks, token_type_ids, labels\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:37:40.573831Z","iopub.execute_input":"2024-05-04T17:37:40.574337Z","iopub.status.idle":"2024-05-04T17:37:40.587479Z","shell.execute_reply.started":"2024-05-04T17:37:40.574181Z","shell.execute_reply":"2024-05-04T17:37:40.586470Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\nlabel_dict = {'O': 0, '1': 1, '2': 2}\ninput_ids, attention_masks, token_type_ids, labels = align_tokens_with_diacritics(\n    asci_sentences, sentences_diacritics, tokenizer, label_dict\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:37:40.589196Z","iopub.execute_input":"2024-05-04T17:37:40.589704Z","iopub.status.idle":"2024-05-04T17:38:49.331147Z","shell.execute_reply.started":"2024-05-04T17:37:40.589563Z","shell.execute_reply":"2024-05-04T17:38:49.330334Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Convert lists of labels into a tensor; assuming each label is already converted to a numeric form\nlabel_tensors = torch.tensor(labels, dtype=torch.long)\n\n# Create a TensorDataset\ndataset = TensorDataset(input_ids, attention_masks, token_type_ids, label_tensors)\n\n# You can now use this dataset with a DataLoader to create batches\nfrom torch.utils.data import DataLoader\ndataloader = DataLoader(dataset, batch_size=16, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:38:49.332741Z","iopub.execute_input":"2024-05-04T17:38:49.333045Z","iopub.status.idle":"2024-05-04T17:38:49.539482Z","shell.execute_reply.started":"2024-05-04T17:38:49.333005Z","shell.execute_reply":"2024-05-04T17:38:49.538492Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from transformers import BertForTokenClassification, AdamW\n\nnum_labels = len(set([label for sublist in labels for label in sublist]))\n\"\"\"\nmodel = BertForTokenClassification.from_pretrained(\n    \"dbmdz/bert-base-turkish-cased\",\n    num_labels=num_labels,\n    output_attentions=False,\n    output_hidden_states=False,\n)\n\n# Move model to GPU if available\nmodel.to(device)\n\n# Setting up the optimizer\noptimizer = AdamW(model.parameters(), lr=2e-5)\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:38:49.540818Z","iopub.execute_input":"2024-05-04T17:38:49.541196Z","iopub.status.idle":"2024-05-04T17:38:49.813070Z","shell.execute_reply.started":"2024-05-04T17:38:49.541150Z","shell.execute_reply":"2024-05-04T17:38:49.811996Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"'\\nmodel = BertForTokenClassification.from_pretrained(\\n    \"dbmdz/bert-base-turkish-cased\",\\n    num_labels=num_labels,\\n    output_attentions=False,\\n    output_hidden_states=False,\\n)\\n\\n# Move model to GPU if available\\nmodel.to(device)\\n\\n# Setting up the optimizer\\noptimizer = AdamW(model.parameters(), lr=2e-5)\\n'"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm import tqdm\n\"\"\"\nepochs = 1\n\nmodel.train()\nfor epoch in range(epochs):\n    total_loss = 0\n    for step, batch in enumerate(tqdm(dataloader)):\n        batch = tuple(t.to(device) for t in batch)\n        b_input_ids, b_input_mask, b_token_types, b_labels = batch\n        model.zero_grad()\n        \n        outputs = model(b_input_ids, token_type_ids=b_token_types, attention_mask=b_input_mask, labels=b_labels)\n        loss = outputs[0]\n        total_loss += loss.item()\n        loss.backward()\n        optimizer.step()\n\n    print(f\"Epoch {epoch}, Loss: {total_loss / len(dataloader)}\")\n    \n\"\"\"\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:38:49.814740Z","iopub.execute_input":"2024-05-04T17:38:49.815195Z","iopub.status.idle":"2024-05-04T17:38:49.822689Z","shell.execute_reply.started":"2024-05-04T17:38:49.815129Z","shell.execute_reply":"2024-05-04T17:38:49.821683Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"'\\nepochs = 1\\n\\nmodel.train()\\nfor epoch in range(epochs):\\n    total_loss = 0\\n    for step, batch in enumerate(tqdm(dataloader)):\\n        batch = tuple(t.to(device) for t in batch)\\n        b_input_ids, b_input_mask, b_token_types, b_labels = batch\\n        model.zero_grad()\\n        \\n        outputs = model(b_input_ids, token_type_ids=b_token_types, attention_mask=b_input_mask, labels=b_labels)\\n        loss = outputs[0]\\n        total_loss += loss.item()\\n        loss.backward()\\n        optimizer.step()\\n\\n    print(f\"Epoch {epoch}, Loss: {total_loss / len(dataloader)}\")\\n    \\n'"},"metadata":{}}]},{"cell_type":"code","source":"# Save the model's state dictionary\n# torch.save(model.state_dict(), 'model_state_dict.pth')\n\n# Optionally, save the entire model\n# torch.save(model, 'full_model.pth')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:38:49.823970Z","iopub.execute_input":"2024-05-04T17:38:49.824322Z","iopub.status.idle":"2024-05-04T17:38:49.830135Z","shell.execute_reply.started":"2024-05-04T17:38:49.824270Z","shell.execute_reply":"2024-05-04T17:38:49.829132Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# Input text\ninput_text = \"sinif havuz ve acik deniz calismalariyla tum dunyada\"\n\n# Tokenize the input\ntest_inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\ntest_inputs = {k: v.to(device) for k, v in test_inputs.items()}\n\n# Predict\nmodel.eval()  # Set the model to evaluation mode\nwith torch.no_grad():\n    outputs = model(**test_inputs)\n    predictions = torch.argmax(outputs.logits, dim=-1)\n\n# Get tokens for reference\ntokens = tokenizer.convert_ids_to_tokens(test_inputs['input_ids'][0])\n\n# Define diacritics mapping\ndiacritics_mapping = {\n    'c': {1: 'ç'},\n    'g': {1: 'ğ'},\n    'i': {1: 'ı'},\n    'o': {1: 'ö'},\n    's': {1: 'ş'},\n    'u': {1: 'ü'}\n}\n\n# Function to apply diacritics based on predictions\ndef apply_diacritics(tokens, predictions, mapping):\n    diacritized_text = []\n    skip_tokens = ['[CLS]', '[SEP]', '[PAD]']  # Tokens to skip when applying diacritics\n\n    # Accumulator for reassembling subword tokens\n    current_word = \"\"\n    for token, pred in zip(tokens, predictions[0]):\n        if token in skip_tokens:\n            continue\n\n        # Check if the token is a continuation of the previous one\n        if token.startswith(\"##\"):\n            current_word += token[2:]  # Append without \"##\" and without adding space\n        else:\n            # If there's accumulated word data from previous tokens, append it first\n            if current_word:\n                diacritized_text.append(current_word)\n                current_word = \"\"\n            # Start a new word accumulation\n            current_word = token\n\n        # Apply diacritics to the current part of the word if applicable\n        if current_word in mapping and pred.item() in mapping[current_word]:\n            # Replace the whole word with its diacritized version\n            current_word = mapping[current_word][pred.item()]\n\n    # Append the last accumulated word if any\n    if current_word:\n        diacritized_text.append(current_word)\n\n    return \" \".join(diacritized_text)\n\n# Apply diacritics\ndiacritized_output = apply_diacritics(tokens, predictions, diacritics_mapping)\nprint(predictions)\nprint(\"Original:\", input_text)\nprint(\"Diacritized:\", diacritized_output)\n\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:38:49.831882Z","iopub.execute_input":"2024-05-04T17:38:49.832282Z","iopub.status.idle":"2024-05-04T17:38:49.843051Z","shell.execute_reply.started":"2024-05-04T17:38:49.832207Z","shell.execute_reply":"2024-05-04T17:38:49.842317Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"'\\n# Input text\\ninput_text = \"sinif havuz ve acik deniz calismalariyla tum dunyada\"\\n\\n# Tokenize the input\\ntest_inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\\ntest_inputs = {k: v.to(device) for k, v in test_inputs.items()}\\n\\n# Predict\\nmodel.eval()  # Set the model to evaluation mode\\nwith torch.no_grad():\\n    outputs = model(**test_inputs)\\n    predictions = torch.argmax(outputs.logits, dim=-1)\\n\\n# Get tokens for reference\\ntokens = tokenizer.convert_ids_to_tokens(test_inputs[\\'input_ids\\'][0])\\n\\n# Define diacritics mapping\\ndiacritics_mapping = {\\n    \\'c\\': {1: \\'ç\\'},\\n    \\'g\\': {1: \\'ğ\\'},\\n    \\'i\\': {1: \\'ı\\'},\\n    \\'o\\': {1: \\'ö\\'},\\n    \\'s\\': {1: \\'ş\\'},\\n    \\'u\\': {1: \\'ü\\'}\\n}\\n\\n# Function to apply diacritics based on predictions\\ndef apply_diacritics(tokens, predictions, mapping):\\n    diacritized_text = []\\n    skip_tokens = [\\'[CLS]\\', \\'[SEP]\\', \\'[PAD]\\']  # Tokens to skip when applying diacritics\\n\\n    # Accumulator for reassembling subword tokens\\n    current_word = \"\"\\n    for token, pred in zip(tokens, predictions[0]):\\n        if token in skip_tokens:\\n            continue\\n\\n        # Check if the token is a continuation of the previous one\\n        if token.startswith(\"##\"):\\n            current_word += token[2:]  # Append without \"##\" and without adding space\\n        else:\\n            # If there\\'s accumulated word data from previous tokens, append it first\\n            if current_word:\\n                diacritized_text.append(current_word)\\n                current_word = \"\"\\n            # Start a new word accumulation\\n            current_word = token\\n\\n        # Apply diacritics to the current part of the word if applicable\\n        if current_word in mapping and pred.item() in mapping[current_word]:\\n            # Replace the whole word with its diacritized version\\n            current_word = mapping[current_word][pred.item()]\\n\\n    # Append the last accumulated word if any\\n    if current_word:\\n        diacritized_text.append(current_word)\\n\\n    return \" \".join(diacritized_text)\\n\\n# Apply diacritics\\ndiacritized_output = apply_diacritics(tokens, predictions, diacritics_mapping)\\nprint(predictions)\\nprint(\"Original:\", input_text)\\nprint(\"Diacritized:\", diacritized_output)\\n\\n'"},"metadata":{}}]},{"cell_type":"code","source":"class BertBiLSTMCRF(nn.Module):\n    def __init__(self, bert_model_name, num_tags):\n        super(BertBiLSTMCRF, self).__init__()\n        self.bert = BertModel.from_pretrained(bert_model_name)\n        self.lstm = nn.LSTM(input_size=768, hidden_size=256, num_layers=1, bidirectional=True, batch_first=True)\n        self.fc = nn.Linear(512, num_tags)  # Adjust based on your number of tags\n        self.crf = CRF(num_tags, batch_first=True)\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        # Use your existing variables for BERT model processing\n        bert_output = self.bert(input_ids, attention_mask=attention_mask)[0]\n        lstm_output, _ = self.lstm(bert_output)\n        emissions = self.fc(lstm_output)\n\n        if labels is not None:\n            # Calculate loss if labels are provided\n            loss = -self.crf(emissions, labels, mask=attention_mask.byte())\n            return loss\n        else:\n            # Decode the predicted sequence of tags if no labels are provided\n            return self.crf.decode(emissions, mask=attention_mask.byte())","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:40:30.499732Z","iopub.execute_input":"2024-05-04T17:40:30.500245Z","iopub.status.idle":"2024-05-04T17:40:30.511258Z","shell.execute_reply.started":"2024-05-04T17:40:30.500159Z","shell.execute_reply":"2024-05-04T17:40:30.510234Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"\"\"\"\n# Initialize the model and move it to the appropriate device\nmodel = BertBiLSTMCRF('dbmdz/bert-base-turkish-cased', num_tags=num_labels)\nmodel = model.to(device)\n\n# Define the optimizer\noptimizer = AdamW(model.parameters(), lr=5e-5)\nnum_epochs = 1\n# Setup the learning rate scheduler\n# total_steps = len(dataloader) * num_epochs  # Adjust according to your actual dataloader and num_epochs\ntotal_steps = 10\nscheduler = get_linear_schedule_with_warmup(optimizer,\n                                            num_warmup_steps=0,\n                                            num_training_steps=total_steps)\n\n# Training Loop\nfor epoch in range(num_epochs):\n    model.train()  # Put the model into training mode\n    total_loss = 0\n\n    for batch in dataloader:\n        batch = tuple(t.to(device) for t in batch)  # Move batch to the correct device\n        input_ids, attention_mask, labels = batch[0], batch[1], batch[3]  # Adjust indexing if necessary\n\n        model.zero_grad()  # Clear existing gradients\n        \n        # Forward pass to get loss\n        loss = model(input_ids, attention_mask, labels)\n        print(loss.item())\n        \n        # Backward pass to calculate the gradients\n        loss.backward()\n        \n        # Track total loss for logging\n        total_loss += loss.item()\n        \n        # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        # Update model parameters\n        optimizer.step()\n        scheduler.step()  # Update the learning rate\n\n    # Output the average loss for the epoch\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\n\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:50:59.852038Z","iopub.execute_input":"2024-05-04T17:50:59.852430Z","iopub.status.idle":"2024-05-04T17:50:59.859848Z","shell.execute_reply.started":"2024-05-04T17:50:59.852386Z","shell.execute_reply":"2024-05-04T17:50:59.859000Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"'\\n# Initialize the model and move it to the appropriate device\\nmodel = BertBiLSTMCRF(\\'dbmdz/bert-base-turkish-cased\\', num_tags=num_labels)\\nmodel = model.to(device)\\n\\n# Define the optimizer\\noptimizer = AdamW(model.parameters(), lr=5e-5)\\nnum_epochs = 1\\n# Setup the learning rate scheduler\\n# total_steps = len(dataloader) * num_epochs  # Adjust according to your actual dataloader and num_epochs\\ntotal_steps = 10\\nscheduler = get_linear_schedule_with_warmup(optimizer,\\n                                            num_warmup_steps=0,\\n                                            num_training_steps=total_steps)\\n\\n# Training Loop\\nfor epoch in range(num_epochs):\\n    model.train()  # Put the model into training mode\\n    total_loss = 0\\n\\n    for batch in dataloader:\\n        batch = tuple(t.to(device) for t in batch)  # Move batch to the correct device\\n        input_ids, attention_mask, labels = batch[0], batch[1], batch[3]  # Adjust indexing if necessary\\n\\n        model.zero_grad()  # Clear existing gradients\\n        \\n        # Forward pass to get loss\\n        loss = model(input_ids, attention_mask, labels)\\n        print(loss.item())\\n        \\n        # Backward pass to calculate the gradients\\n        loss.backward()\\n        \\n        # Track total loss for logging\\n        total_loss += loss.item()\\n        \\n        # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\\n        \\n        # Update model parameters\\n        optimizer.step()\\n        scheduler.step()  # Update the learning rate\\n\\n    # Output the average loss for the epoch\\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(dataloader)}\")\\n'"},"metadata":{}}]},{"cell_type":"code","source":"# Input text\ninput_text = \"sinif havuz ve acik deniz calismalariyla tum dunyada\"\n\n# Tokenize the input\ntest_inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\ntest_inputs = {k: v.to(device) for k, v in test_inputs.items() if k in ['input_ids', 'attention_mask']}\n\n# Predict\nmodel.eval()  # Set the model to evaluation mode\nwith torch.no_grad():\n    # Since CRF outputs the best tag sequence directly, there is no need for argmax\n    predictions = model(**test_inputs)\n\n# Get tokens for reference\ntokens = tokenizer.convert_ids_to_tokens(test_inputs['input_ids'][0])\n\n# Define diacritics mapping\ndiacritics_mapping = {\n    'c': {1: 'ç'},\n    'g': {1: 'ğ'},\n    'i': {1: 'ı'},\n    'o': {1: 'ö'},\n    's': {1: 'ş'},\n    'u': {1: 'ü'}\n}\n\n# Function to apply diacritics based on predictions\ndef apply_diacritics(tokens, predictions, mapping):\n    diacritized_text = []\n    skip_tokens = ['[CLS]', '[SEP]', '[PAD]']  # Tokens to skip when applying diacritics\n\n    # Accumulator for reassembling subword tokens\n    current_word = \"\"\n    for token, pred in zip(tokens, predictions[0]):\n        if token in skip_tokens:\n            continue\n\n        # Check if the token is a continuation of the previous one\n        if token.startswith(\"##\"):\n            current_word += token[2:]  # Append without \"##\" and without adding space\n        else:\n            # If there's accumulated word data from previous tokens, append it first\n            if current_word:\n                diacritized_text.append(current_word)\n                current_word = \"\"\n            # Start a new word accumulation\n            current_word = token\n\n        # Apply diacritics to the current part of the word if applicable\n        if current_word in mapping and pred in mapping[current_word]:\n            # Replace the whole word with its diacritized version\n            current_word = mapping[current_word][pred]\n\n    # Append the last accumulated word if any\n    if current_word:\n        diacritized_text.append(current_word)\n\n    return \" \".join(diacritized_text)\n\n# Apply diacritics\ndiacritized_output = apply_diacritics(tokens, predictions, diacritics_mapping)\nprint(\"Predictions:\", predictions)\nprint(\"Original:\", input_text)\nprint(\"Diacritized:\", diacritized_output)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-04T17:50:13.351024Z","iopub.execute_input":"2024-05-04T17:50:13.351622Z","iopub.status.idle":"2024-05-04T17:50:13.384164Z","shell.execute_reply.started":"2024-05-04T17:50:13.351555Z","shell.execute_reply":"2024-05-04T17:50:13.383331Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"Predictions: [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\nOriginal: sinif havuz ve acik deniz calismalariyla tum dunyada\nDiacritized: sinif havuz ve acik deniz calismalariyla tum dunyada\n","output_type":"stream"}]}]}