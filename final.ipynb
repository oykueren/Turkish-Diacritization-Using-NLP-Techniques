{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mustafa/miniconda3/envs/new_test/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from unidecode import unidecode\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn as nn\n",
    "from torchcrf import CRF\n",
    "from tqdm import tqdm\n",
    "\n",
    "from zemberek.morphology import TurkishMorphology\n",
    "from zemberek.normalization import TurkishSpellChecker\n",
    "\n",
    "from vnlp import Normalizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pytorch-crf\n",
    "# !pip install unidecode\n",
    "# !pip install --upgrade transformers\n",
    "# !pip install --upgrade torch\n",
    "# !pip install zemberek-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, using the CPU instead.\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():  \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    \n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_train=pd.read_csv(\"train.csv\", index_col=[0])\n",
    "nlp_test=pd.read_csv(\"test.csv\",index_col=[0],encoding=\"utf-8\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Dictionaries to use on Data Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to lowerize the letters\n",
    "lower_chars_dict = {\n",
    "    \"A\": \"a\",\n",
    "    \"B\": \"b\",\n",
    "    \"C\": \"c\",\n",
    "    \"Ç\": \"ç\",\n",
    "    \"D\": \"d\",\n",
    "    \"E\": \"e\",\n",
    "    \"F\": \"f\",\n",
    "    \"G\": \"g\",\n",
    "    \"Ğ\": \"ğ\",\n",
    "    \"H\": \"h\",\n",
    "    \"I\": \"ı\",\n",
    "    \"İ\": \"i\",\n",
    "    \"J\": \"j\",\n",
    "    \"K\": \"k\",\n",
    "    \"L\": \"l\",\n",
    "    \"M\": \"m\",\n",
    "    \"N\": \"n\",\n",
    "    \"O\": \"o\",\n",
    "    \"Ö\": \"ö\",\n",
    "    \"P\": \"p\",\n",
    "    \"R\": \"r\",\n",
    "    \"S\": \"s\",\n",
    "    \"Ş\": \"ş\",\n",
    "    \"T\": \"t\",\n",
    "    \"U\": \"u\",\n",
    "    \"Ü\": \"ü\",\n",
    "    \"V\": \"v\",\n",
    "    \"Y\": \"y\",\n",
    "    \"Z\": \"z\",\n",
    "    \"Q\": \"q\",\n",
    "    \"W\": \"w\",\n",
    "    \"X\": \"x\",\n",
    "    \"Â\": \"â\",\n",
    "    \"Ê\": \"ê\",\n",
    "    \"Î\": \"î\",\n",
    "    \"Û\": \"û\",\n",
    "    \"Ô\": \"ô\"\n",
    "    }\n",
    "# to convert ascii format\n",
    "words_dict = {\n",
    "    \"ı\": \"i\",\n",
    "    \"ğ\": \"g\",\n",
    "    \"ü\": \"u\",\n",
    "    \"ş\": \"s\",\n",
    "    \"ö\": \"o\",\n",
    "    \"ç\": \"c\",\n",
    "    \"İ\": \"I\",\n",
    "    \"Ğ\": \"G\",\n",
    "    \"Ü\": \"U\",\n",
    "    \"Ş\": \"S\",\n",
    "    \"Ö\": \"O\",\n",
    "    \"Ç\": \"C\"\n",
    "    }\n",
    "\n",
    "# to numericize the letters\n",
    "characters_dictionary = {\n",
    "    \"a\":1,\n",
    "    \"b\":2,\n",
    "    \"c\":3,\n",
    "    \"ç\":4,\n",
    "    \"d\":5,\n",
    "    \"e\":6,\n",
    "    \"f\":7,\n",
    "    \"g\":8,\n",
    "    \"ğ\":9,\n",
    "    \"h\":10,\n",
    "    \"ı\":11,\n",
    "    \"i\":12,\n",
    "    \"j\":13,\n",
    "    \"k\":14,\n",
    "    \"l\":15,\n",
    "    \"m\":16,\n",
    "    \"n\":17,\n",
    "    \"o\":18,\n",
    "    \"ö\":19,\n",
    "    \"p\":20,\n",
    "    \"r\":21,\n",
    "    \"s\":22,\n",
    "    \"ş\":23,\n",
    "    \"t\":24,\n",
    "    \"u\":25,\n",
    "    \"ü\":26,\n",
    "    \"v\":27,\n",
    "    \"y\":28,\n",
    "    \"z\":29,\n",
    "    \"q\":30,\n",
    "    \"x\":31,\n",
    "    \"w\":32,\n",
    "    \" \":33,\n",
    "    \"â\":34,\n",
    "    \"ê\":35,\n",
    "    \"î\":36,\n",
    "    \"û\":37,\n",
    "    \"ô\":38\n",
    "    }\n",
    "\n",
    "# to ckeck diacritic or non-diacritic versions of letters on zemberek process\n",
    "diacritic_versions = {\"i\":\"ı\", \"ı\":\"i\", \"o\":\"ö\", \"ö\":\"o\", \"u\":\"ü\", \"ü\":\"u\", \"g\":\"ğ\", \"ğ\":\"g\", \"s\":\"ş\", \"ş\":\"s\", \"c\":\"ç\", \"ç\":\"c\", \"I\":\"İ\", \"İ\":\"I\", \n",
    "                      \"O\":\"Ö\", \"Ö\":\"O\", \"U\":\"Ü\", \"Ü\":\"U\", \"G\":\"Ğ\", \"Ğ\":\"G\", \"S\":\"Ş\", \"Ş\":\"S\", \"C\":\"Ç\", \"Ç\":\"C\", \"â\":\"a\", \"a\":\"â\", \"ê\":\"e\", \"e\":\"ê\",\n",
    "                      \"î\":\"ı\", \"ı\":\"î\", \"û\":\"u\", \"u\":\"û\", \"ô\":\"o\", \"o\":\"ô\", \"I\":\"Î\", \"Î\":\"I\", \"U\":\"Û\", \"Û\":\"U\", \"O\":\"Ô\", \"Ô\":\"O\", \"Â\":\"A\", \"A\":\"Â\",\n",
    "                      \"Ê\":\"E\", \"E\":\"Ê\"}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREPROCESS OF TRAIN DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ascii(sentence):\n",
    "    # convert Turkish characters to English characters\n",
    "    for key, value in words_dict.items():\n",
    "        sentence = sentence.replace(key, value)\n",
    "    return sentence\n",
    "\n",
    "def remove_puncutations_and_numbers(text):\n",
    "    # iterate over the string and remove char if it is not a character\n",
    "    characters = \"abcçdefgğhıijklmnoöprsştuüvyzABCÇDEFGĞHIİJKLMNOÖPRSŞTUÜVYZxXwWqQâêîôûÂÊÎÔÛ \"\n",
    "    for char in text:\n",
    "        if char not in characters:\n",
    "            text = text.replace(char, \"\")\n",
    "    # remove multiple spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    return text\n",
    "\n",
    "# split the sentences into smaller sentences\n",
    "def split_sentences(sentences, max_length=99):\n",
    "    punctuations = {'.', '?', '!', ';', ':', ','}\n",
    "    results = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        while len(sentence) > max_length:\n",
    "            # find last punctuation before max_length\n",
    "            split_pos = -1\n",
    "            for p in punctuations:\n",
    "                pos = sentence.rfind(p, 0, max_length + 1)\n",
    "                if pos > split_pos:\n",
    "                    split_pos = pos\n",
    "            \n",
    "            # if no punctuation found, split at the last space before max_length\n",
    "            if split_pos == -1:\n",
    "                split_pos = sentence.rfind(' ', 0, max_length + 1)\n",
    "            \n",
    "            # if no space found, just split at max_length\n",
    "            if split_pos == -1:\n",
    "                split_pos = max_length\n",
    "            \n",
    "            # append the split segment to results\n",
    "            results.append(sentence[:split_pos + 1].strip())\n",
    "            # move the rest of the sentence forward\n",
    "            sentence = sentence[split_pos + 1:].strip()\n",
    "        \n",
    "        # append the remainder of the sentence if it's not empty\n",
    "        if sentence:\n",
    "            results.append(sentence)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# padding function\n",
    "def padding(text, filling_char, max_length):\n",
    "    if type(text) is str:\n",
    "        text = text + filling_char * (max_length - len(text))\n",
    "    elif type(text) is list:\n",
    "        text = text + [filling_char] * (max_length - len(text))\n",
    "    return text\n",
    "\n",
    "# map the diacritics to numbers\n",
    "def map_diacritics(text):\n",
    "    for char_index in range(len(text)):\n",
    "        # print(text[char_index])\n",
    "        if text[char_index] in \"ıöüğşç\":\n",
    "            text[char_index] = 2\n",
    "        elif text[char_index] in \"aeiougsc\":\n",
    "            text[char_index] = 1\n",
    "        elif text[char_index] == \" \":\n",
    "            text[char_index] = 0\n",
    "        elif text[char_index] in \"âêîôû\":\n",
    "            text[char_index] = 4\n",
    "        else:\n",
    "            text[char_index] = 3\n",
    "        \n",
    "    return text\n",
    "\n",
    "def prepare_train_dataset(sentences):\n",
    "    processed_sentences = []\n",
    "    # iterate over the sentences\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # remove punctuations and numbers\n",
    "        sentence = remove_puncutations_and_numbers(sentence)\n",
    "        # split the sentence into smaller sentences\n",
    "        splitted_sentences = split_sentences([sentence])\n",
    "        new_list = []\n",
    "        # iterate over the splitted sentences\n",
    "        for s in splitted_sentences:\n",
    "            # lowerize characters\n",
    "            for key, value in lower_chars_dict.items():\n",
    "                s = s.replace(key, value)\n",
    "            new_list.append(s)\n",
    "        processed_sentences.extend(new_list)\n",
    "    \n",
    "    diactrize_labels = processed_sentences.copy()\n",
    "    # iterate over the sentences to diacritize them\n",
    "    for i, sentence in enumerate(processed_sentences):\n",
    "        diactrize_labels[i] = map_diacritics(list(sentence))\n",
    "        # add padding to the diacritized sentence\n",
    "        diactrize_labels[i] = padding(diactrize_labels[i], 0, 100)\n",
    "    # asci sentences \n",
    "    asci_sentences = processed_sentences.copy()\n",
    "    # iterate over the sentences to convert them to asci and map them to numbers\n",
    "    for i, sentence in enumerate(processed_sentences):\n",
    "        sentence = Normalizer.remove_accent_marks(sentence)\n",
    "        asci_sentences[i] = convert_to_ascii(sentence)\n",
    "        \n",
    "    numeric_sentences = []\n",
    "    for i, sentence in enumerate(asci_sentences):\n",
    "        numeric_sentence = []\n",
    "        for char in sentence:\n",
    "            numeric_sentence.append(characters_dictionary[char])\n",
    "        # add padding to the numeric sentence\n",
    "        numeric_sentence = padding(numeric_sentence, 0, 100)\n",
    "        numeric_sentences.append(numeric_sentence)\n",
    "    return processed_sentences, diactrize_labels, asci_sentences, numeric_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "turkish_sentences = [\n",
    "     \"İmrali\", \"merhâbâ\", \"31 - giyim çeşidine göre standart ölçülere ilâve edilecek ölçü tablosu\", \"prof. dr. şükrü halûk akalın ve prof. dr. ali duymaz yönettikleri bölümlerin sonuç bildirilerini okudular.\"\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw sentences\n",
    "raw_sentences = nlp_train[\"Sentence\"].values\n",
    "\n",
    "processed_turkish_sentences,labels,asci_sentences,numeric_sentences = prepare_train_dataset(raw_sentences.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 1, 3, 2, 3, 1, 3, 0, 1, 3, 1, 2, 3, 2, 3, 3, 1, 3, 1, 3, 0, 2, 2, 3, 1, 3, 1, 1, 3, 1, 3, 1, 3, 0, 3, 1, 3, 1, 1, 3, 0, 3, 1, 3, 2, 2, 0, 3, 1, 3, 1, 3, 1, 3, 2, 3, 2, 0, 3, 1, 3, 1, 3, 3, 1, 3, 2, 3, 3, 1, 3, 0, 1, 1, 3, 3, 1, 0, 3, 1, 3, 1, 0, 3, 1, 3, 3, 1, 3, 1, 3, 1, 3, 1, 0, 0, 0, 0, 0, 0]\n",
      "yapilan arastirmalar ogrencilerin mevcut dalis kurslarini tamamladiktan sonra bile kendilerini\n",
      "[28, 1, 20, 12, 15, 1, 17, 33, 1, 21, 1, 22, 24, 12, 21, 16, 1, 15, 1, 21, 33, 18, 8, 21, 6, 17, 3, 12, 15, 6, 21, 12, 17, 33, 16, 6, 27, 3, 25, 24, 33, 5, 1, 15, 12, 22, 33, 14, 25, 21, 22, 15, 1, 21, 12, 17, 12, 33, 24, 1, 16, 1, 16, 15, 1, 5, 12, 14, 24, 1, 17, 33, 22, 18, 17, 21, 1, 33, 2, 12, 15, 6, 33, 14, 6, 17, 5, 12, 15, 6, 21, 12, 17, 12, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "n=3\n",
    "print(labels[n])\n",
    "print(asci_sentences[n])\n",
    "print(numeric_sentences[n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mustafa/miniconda3/envs/new_test/lib/python3.11/site-packages/torch/nn/modules/transformer.py:306: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DiacritizationBiLSTMCRF(\n",
       "  (embedding): Embedding(39, 128)\n",
       "  (transformer_layer): TransformerEncoderLayer(\n",
       "    (self_attn): MultiheadAttention(\n",
       "      (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "    (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout1): Dropout(p=0.1, inplace=False)\n",
       "    (dropout2): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-4): 5 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=2048, bias=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (linear2): Linear(in_features=2048, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (dropout2): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (lstm_enc): LSTM(128, 128, batch_first=True, bidirectional=True)\n",
       "  (lstm_dec): LSTM(256, 256, batch_first=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc): Linear(in_features=512, out_features=5, bias=True)\n",
       "  (crf): CRF(num_tags=5)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DiacritizationBiLSTMCRF(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim_enc, hidden_dim_dec, num_labels, num_heads, num_layers):\n",
    "        super(DiacritizationBiLSTMCRF, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.transformer_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(self.transformer_layer, num_layers=num_layers)\n",
    "        self.lstm_enc = nn.LSTM(embed_dim, hidden_dim_enc // 2, num_layers=1, bidirectional=True, batch_first=True)\n",
    "        self.lstm_dec = nn.LSTM(hidden_dim_enc, hidden_dim_dec, num_layers=1, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(hidden_dim_dec * 2, num_labels) \n",
    "        self.crf = CRF(num_labels, batch_first=True)\n",
    "\n",
    "    def forward(self, sentences, labels=None):\n",
    "        x = self.embedding(sentences)\n",
    "        \n",
    "        # Transformer layer\n",
    "        x = x.permute(1, 0, 2) \n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.permute(1, 0, 2) \n",
    "        \n",
    "        # BiLSTM encoder layer\n",
    "        enc_output, _ = self.lstm_enc(x)\n",
    "        \n",
    "        # Decoder LSTM\n",
    "        dec_outputs, _ = self.lstm_dec(enc_output)\n",
    "        \n",
    "        combined_outputs = torch.cat((enc_output, dec_outputs), dim=2)\n",
    "        \n",
    "        emissions = self.fc(combined_outputs)\n",
    "        \n",
    "        # CRF layer\n",
    "        if labels is not None:\n",
    "            # if labels are provided, calculate the loss\n",
    "            loss = -self.crf(emissions, labels)\n",
    "            return loss\n",
    "        else:\n",
    "            # otherwise, return the best path\n",
    "            prediction = self.crf.decode(emissions)\n",
    "            return prediction\n",
    "\n",
    "vocab_size = len(characters_dictionary) + 1 \n",
    "embed_dim = 128\n",
    "hidden_dim_enc = 256\n",
    "hidden_dim_dec = 256\n",
    "num_heads = 8\n",
    "num_labels = 5\n",
    "num_layers = 5\n",
    "\n",
    "model = DiacritizationBiLSTMCRF(vocab_size, embed_dim, hidden_dim_enc, hidden_dim_dec, num_labels, num_heads, num_layers)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_sentences_tensor = torch.tensor(numeric_sentences, dtype=torch.long)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "# Create TensorDataset and DataLoader for training and validation\n",
    "train_dataset = TensorDataset(numeric_sentences_tensor, labels_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raining the model\n",
    "def train_model(model, train_loader, optimizer, num_epochs=100):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        pbar = tqdm(train_loader, total=len(train_loader), leave=False)\n",
    "        for sentences, label_seqs in pbar:\n",
    "            sentences, label_seqs = sentences.to(device), label_seqs.to(device)\n",
    "            \n",
    "            # Forward pass: Compute predicted y by passing x to the model\n",
    "            loss = model(sentences, labels=label_seqs)\n",
    "            \n",
    "            # Zero gradients, perform a backward pass, and update the weights.\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            pbar.set_description(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss / len(pbar):.4f}\")\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        \n",
    "        torch.save(model.state_dict(), f'/home/oeren/Documents/YZV-NLP/weights_final2/model_epoch_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "#train_model(model, train_loader, optimizer, num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POST PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to improve the results with ZEMBEREK\n",
    "def check_morphology(sentences):\n",
    "    morphology = TurkishMorphology.create_with_defaults()\n",
    "    morphology.ignoreDiacriticsInAnalysis = False\n",
    "    spellChecker = TurkishSpellChecker(morphology)\n",
    "\n",
    "    corrected_sentences = sentences.copy()\n",
    "\n",
    "    # Iterate through each sentence\n",
    "    for i in range(len(corrected_sentences)):\n",
    "        # split the sentence into words and iterate through each word\n",
    "        words = corrected_sentences[i].split()\n",
    "        for m in range(len(words)):\n",
    "            if \"\\u00B0\" in words[m] or len(words[m]) == 1:\n",
    "                continue\n",
    "            # analyze the word\n",
    "            analysis = morphology.analyze(words[m])\n",
    "\n",
    "            # check if the word has no analysis results\n",
    "            if len(analysis.analysis_results) == 0:\n",
    "                print(\"No analysis results for:\", words[m])\n",
    "                \n",
    "                # check if there are suggestions from the spell checker\n",
    "                suggestions = spellChecker.suggest_for_word(words[m])\n",
    "                if len(suggestions) != 0:\n",
    "                    for suggested_word in suggestions:\n",
    "                        print(\"Suggested word:\", suggested_word)                  \n",
    "                        # if the suggested word has the same length as the original word, and just \"ıioöuügğsşcçâêîôûae\" characters are different, replace the word\n",
    "                        if len(suggested_word) == len(words[m]):\n",
    "                            for char1, char2 in zip(suggested_word, words[m]):\n",
    "                                if char1 in diacritic_versions and char2 == diacritic_versions[char1]:\n",
    "                                    continue\n",
    "                                if char1 in diacritic_versions and char2 == char1:\n",
    "                                    continue\n",
    "                                if char1 not in diacritic_versions and char1 == char2:\n",
    "                                    continue\n",
    "                                elif char1 in diacritic_versions and char2 != diacritic_versions[char1] and char2 != char1:\n",
    "                                    break\n",
    "                                elif char1 not in diacritic_versions and char1 != char2:\n",
    "                                    break\n",
    "                            else:\n",
    "                                print(\"Suggested word is approved:\", suggested_word)\n",
    "                                words[m] = suggested_word\n",
    "                                break\n",
    "\n",
    "        # join the modified words back into a sentence\n",
    "        corrected_sentences[i] = ' '.join(words)\n",
    "\n",
    "    return corrected_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions to sentences\n",
    "def predict_test_sentence(sentence, model):\n",
    "    # strip the sentence\n",
    "    sentence = sentence.strip()\n",
    "    original_sentence = sentence\n",
    "    sentence = remove_puncutations_and_numbers(sentence)    \n",
    "    # lowerize characters\n",
    "    for key, value in lower_chars_dict.items():\n",
    "        sentence = sentence.replace(key, value)\n",
    "    \n",
    "    # convert to asci\n",
    "    sentence = Normalizer.remove_accent_marks(sentence)\n",
    "    sentence = convert_to_ascii(sentence)\n",
    "    \n",
    "    sentences_array = []\n",
    "    # split sentence into smaller sentences by using split_sentences function\n",
    "    sentences_array.extend(split_sentences([sentence]))\n",
    "    # map to numbers\n",
    "    numeric_sentences = []\n",
    "    for sentence in sentences_array:\n",
    "        numeric_sentence = []\n",
    "        # print(len(sentence))\n",
    "        for char in sentence:\n",
    "            numeric_sentence.append(characters_dictionary[char])\n",
    "        # add padding\n",
    "        numeric_sentence = padding(numeric_sentence, 0, 100)\n",
    "        numeric_sentences.append(numeric_sentence)\n",
    "    \n",
    "    # make predictions using the model\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    for numeric_sentence in numeric_sentences:\n",
    "        numeric_sentence = torch.tensor([numeric_sentence], dtype=torch.long).to(device)\n",
    "        prediction = model(numeric_sentence)\n",
    "        predictions.extend(prediction[0])\n",
    "    # print(predictions)\n",
    "    # merge predictions to single list and remove 0s from the list\n",
    "    clipped_predictions = [x for x in predictions if x != 0]\n",
    "    # print(len(predictions))\n",
    "    diacritics_map_dict = {\"i\": \"ı\", \"o\": \"ö\", \"u\": \"ü\", \"g\": \"ğ\", \"s\": \"ş\", \"c\": \"ç\", \"I\": \"İ\", \"O\": \"Ö\", \"U\": \"Ü\", \"G\": \"Ğ\", \"S\": \"Ş\", \"C\": \"Ç\"}\n",
    "    diacritic_map_accent = {\"ı\": \"î\", \"a\": \"â\", \"e\": \"ê\", \"o\": \"ô\", \"u\": \"û\", \"I\": \"Î\", \"A\": \"Â\", \"E\": \"Ê\", \"O\": \"Ô\", \"U\": \"Û\"}\n",
    "    # iterate over the original sentence\n",
    "    output_sentence = \"\"\n",
    "    predictions_index = 0\n",
    "    for ind,char in enumerate(original_sentence):\n",
    "        if (char in characters_dictionary  or char in lower_chars_dict) and char != \" \":\n",
    "            # print(\"char:\", char, \"prediction:\", clipped_predictions[predictions_index])\n",
    "            if clipped_predictions[predictions_index] == 2:\n",
    "                if char in diacritics_map_dict:\n",
    "                    if char == \"I\":\n",
    "                        output_sentence += \"I\"\n",
    "                    else:\n",
    "                        output_sentence += diacritics_map_dict[char]\n",
    "                else:\n",
    "                    output_sentence += char\n",
    "            elif clipped_predictions[predictions_index] == 1:\n",
    "                if char == \"I\":\n",
    "                    output_sentence += \"İ\"\n",
    "                else:\n",
    "                    output_sentence += char\n",
    "            elif clipped_predictions[predictions_index] == 4:\n",
    "                if char in diacritic_map_accent:\n",
    "                    output_sentence += diacritic_map_accent[char]\n",
    "                else:\n",
    "                    output_sentence += char\n",
    "            elif clipped_predictions[predictions_index] == 3:\n",
    "                output_sentence += char\n",
    "            predictions_index += 1\n",
    "            \n",
    "        else:\n",
    "            output_sentence += char\n",
    "\n",
    "    return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Îyi misin Mit gôrûsmêsi ihtiyâc dûyûldûkcâ ôlûyôr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mustafa/miniconda3/envs/new_test/lib/python3.11/site-packages/torchcrf/__init__.py:305: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorCompare.cpp:530.)\n",
      "  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"
     ]
    }
   ],
   "source": [
    "uzun_cumle = \"Iyi misin Mit gorusmesi ihtiyac duyuldukca oluyor\"\n",
    "new_sentence = predict_test_sentence(uzun_cumle,model)\n",
    "print(new_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DiacritizationBiLSTMCRF(vocab_size, embed_dim, hidden_dim_enc, hidden_dim_dec, num_labels, num_heads, num_layers)\n",
    "model.to(device)\n",
    "# Load the saved weights\n",
    "model.load_state_dict(torch.load('best.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions on the test set\n",
    "test_sentences = nlp_test[\"Sentence\"].values\n",
    "# iterate over the test sentences and make predictions\n",
    "predicted_sentences = []\n",
    "for sentence in test_sentences:\n",
    "    predicted_sentence = predict_test_sentence(sentence, model)\n",
    "    predicted_sentences.append(predicted_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zemberek_predicted_sentences = check_morphology(predicted_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE THE RESULT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predictions to a CSV file, it will have two columns: \"ID\" and \"Sentence\"\n",
    "output_df = pd.DataFrame({\"ID\": nlp_test.index, \"Sentence\": zemberek_predicted_sentences})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the dataframe to a CSV file\n",
    "# output_df.to_csv(\"predictions4.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
