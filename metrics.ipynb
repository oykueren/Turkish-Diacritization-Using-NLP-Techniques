{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the predictions\n",
    "predictions = pd.read_csv('/home/oeren/Documents/YZV-NLP/results/with2zemberek.csv',  index_col=[0])\n",
    "ground_truth = pd.read_csv('/home/oeren/Documents/YZV-NLP/results/test_gold.csv',  index_col=[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CALCULATE ACCURACY BASED ON WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_overall(test_result, testgold):\n",
    "  correct = 0\n",
    "  total = 0\n",
    "  # count number of correctly diacritized words\n",
    "  for i in range(len(testgold)):\n",
    "    for m in range(len(testgold[i].split())):\n",
    "      if test_result[i].split()[m] == testgold[i].split()[m]:\n",
    "        correct += 1\n",
    "      total +=1\n",
    "\n",
    "  return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_word = acc_overall(predictions['Sentence'], ground_truth['Sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9670433145009416"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CALCULATE DIACRTIZATION ERROR RATE (DER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate the accuracy based on the character level\n",
    "def DER(test_result, testgold):\n",
    "  wrong = 0\n",
    "  total = 0\n",
    "  # count number of correctly diacritized characters\n",
    "  for i in range(len(testgold)):\n",
    "    for m in range(len(testgold[i].split())):\n",
    "      for j in range(len(testgold[i].split()[m])):\n",
    "        if testgold[i].split()[m][j] in \"ıioöuügğsşcçâêîôûaeIİOÖUÜGĞSŞCÇÂÊÎÔÛAE\":\n",
    "          if test_result[i].split()[m][j] != testgold[i].split()[m][j]:\n",
    "            wrong += 1\n",
    "          total +=1\n",
    "\n",
    "  return wrong / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "der_result = DER(predictions['Sentence'], ground_truth['Sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.014442993095726815"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "der_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CALCULATE WORD ERROR RATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WER(test_result, testgold):\n",
    "  wrong = 0\n",
    "  total = 0\n",
    "  wrong_words = []\n",
    "  # count number of correctly diacritized words\n",
    "  for i in range(len(testgold)):\n",
    "    for m in range(len(testgold[i].split())):\n",
    "      if test_result[i].split()[m] != testgold[i].split()[m]:\n",
    "        wrong_words.append([testgold[i].split()[m], test_result[i].split()[m]])\n",
    "        wrong += 1\n",
    "      total +=1\n",
    "\n",
    "  return wrong / total, wrong_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_result, wrong_words = WER(predictions['Sentence'], ground_truth['Sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03295668549905838"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wer_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create CSV With Wrong Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframe for wrong words\n",
    "wrong_words_df = pd.DataFrame(wrong_words, columns=['label', 'prediction'])\n",
    "wrong_words_df.to_csv('/home/oeren/Documents/YZV-NLP/results/wrong_words2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CALCULATE OTHER METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.98\n",
      "Recall: 0.96\n",
      "F1 Score: 0.97\n"
     ]
    }
   ],
   "source": [
    "def calculate_metrics(test_result, testgold):\n",
    "    # Ensure the sequences are of the same length\n",
    "    assert len(test_result) == len(testgold), \"Sequences must be of the same length\"\n",
    "    \n",
    "    # Initialize counts\n",
    "    TP = FP = FN = 0\n",
    "    for i in range(len(testgold)):\n",
    "        for m in range(len(testgold[i].split())):\n",
    "            # Iterate over characters in the sequences\n",
    "            for pred_char, true_char in zip(test_result[i].split()[m], testgold[i].split()[m]):\n",
    "                if pred_char == true_char and pred_char in 'ıöüğşçâêîôûIÖÜĞŞÇÂÊÎÔÛ':\n",
    "                    TP += 1  # True positive: Correctly predicted diacritic character\n",
    "                elif pred_char != true_char and pred_char in 'ıöüğşçâêîôûIÖÜĞŞÇÂÊÎÔÛ':\n",
    "                    FP += 1  # False positive: Incorrectly predicted diacritic character\n",
    "                elif pred_char != true_char and true_char in 'ıöüğşçâêîôûIÖÜĞŞÇÂÊÎÔÛ':\n",
    "                    FN += 1  # False negative: Missed diacritic character\n",
    "            \n",
    "    # Calculate precision, recall, and F1 score\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    return precision, recall, f1_score\n",
    "\n",
    "precision, recall, f1 = calculate_metrics(predictions['Sentence'], ground_truth['Sentence'])\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
