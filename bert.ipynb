{"cells":[{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T14:41:36.793432Z","iopub.status.busy":"2024-05-04T14:41:36.792883Z","iopub.status.idle":"2024-05-04T14:41:51.778706Z","shell.execute_reply":"2024-05-04T14:41:51.777759Z","shell.execute_reply.started":"2024-05-04T14:41:36.793357Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: pytorch-crf in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (0.7.2)\n"]},{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: unidecode in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (1.3.8)\n"]}],"source":["!pip install pytorch-crf\n","!pip install unidecode"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T14:41:51.782030Z","iopub.status.busy":"2024-05-04T14:41:51.781595Z","iopub.status.idle":"2024-05-04T14:41:59.593431Z","shell.execute_reply":"2024-05-04T14:41:59.592406Z","shell.execute_reply.started":"2024-05-04T14:41:51.781964Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (4.40.1)\n","Requirement already satisfied: filelock in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from transformers) (3.14.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from transformers) (0.22.2)\n","Requirement already satisfied: numpy>=1.17 in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from transformers) (24.0)\n","Requirement already satisfied: pyyaml>=5.1 in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from transformers) (2024.4.28)\n","Requirement already satisfied: requests in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from transformers) (0.19.1)\n","Requirement already satisfied: safetensors>=0.4.1 in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from transformers) (0.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from transformers) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.3.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from requests->transformers) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from requests->transformers) (2.2.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install --upgrade transformers"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T14:41:59.596126Z","iopub.status.busy":"2024-05-04T14:41:59.595684Z","iopub.status.idle":"2024-05-04T14:43:06.181865Z","shell.execute_reply":"2024-05-04T14:43:06.180736Z","shell.execute_reply.started":"2024-05-04T14:41:59.596057Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: torch in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (2.3.0)\n","Requirement already satisfied: filelock in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from torch) (3.14.0)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from torch) (4.11.0)\n","Requirement already satisfied: sympy in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from torch) (3.3)\n","Requirement already satisfied: jinja2 in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from torch) (3.1.3)\n","Requirement already satisfied: fsspec in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from torch) (2024.3.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from jinja2->torch) (2.1.5)\n","Requirement already satisfied: mpmath>=0.19 in /Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install --upgrade torch"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T14:43:06.184014Z","iopub.status.busy":"2024-05-04T14:43:06.183662Z","iopub.status.idle":"2024-05-04T14:43:10.077304Z","shell.execute_reply":"2024-05-04T14:43:10.076521Z","shell.execute_reply.started":"2024-05-04T14:43:06.183958Z"},"trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import os\n","import string\n","from unidecode import unidecode\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset, RandomSampler, SequentialSampler\n","from transformers import AdamW, BertModel, AutoConfig, AutoTokenizer\n","from torch.optim import Adam\n","import torch.nn as nn\n","from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n","import time;\n","import datetime\n","from sklearn.model_selection import train_test_split\n","from torchcrf import CRF"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T14:43:10.081510Z","iopub.status.busy":"2024-05-04T14:43:10.081020Z","iopub.status.idle":"2024-05-04T14:43:10.115510Z","shell.execute_reply":"2024-05-04T14:43:10.113891Z","shell.execute_reply.started":"2024-05-04T14:43:10.081447Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["No GPU available, using the CPU instead.\n"]}],"source":["if torch.cuda.is_available():  \n","    device = torch.device(\"cuda\")\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","    \n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T14:43:10.117783Z","iopub.status.busy":"2024-05-04T14:43:10.117395Z","iopub.status.idle":"2024-05-04T14:43:10.252086Z","shell.execute_reply":"2024-05-04T14:43:10.251036Z","shell.execute_reply.started":"2024-05-04T14:43:10.117712Z"},"trusted":true},"outputs":[],"source":["# nlp_train=pd.read_csv(\"/kaggle/input/nlp-project-train/train.csv\", index_col=[0])\n","# nlp_test=pd.read_csv(\"/kaggle/input/nlp-project-train/test.csv\",index_col=[0],encoding=\"windows-1252\") \n","\n","nlp_train=pd.read_csv(\"train.csv\", index_col=[0])\n","nlp_test=pd.read_csv(\"test.csv\",index_col=[0],encoding=\"windows-1252\") "]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T14:43:10.256550Z","iopub.status.busy":"2024-05-04T14:43:10.256193Z","iopub.status.idle":"2024-05-04T14:43:10.268686Z","shell.execute_reply":"2024-05-04T14:43:10.267814Z","shell.execute_reply.started":"2024-05-04T14:43:10.256503Z"},"trusted":true},"outputs":[],"source":["def convert_to_ascii(sentence):\n","    text = unidecode(sentence)\n","    return text\n","\n","def remove_punctuations(text):\n","    for punctuation in string.punctuation:\n","        text = text.replace(punctuation, '')\n","        text = text.replace('  ', ' ')\n","    return text.strip()\n","\n","def split_sentences(sentences, max_length=100):\n","    punctuations = {'.', ',', ';', ':'}\n","    results = []\n","\n","    for sentence in sentences:\n","        while len(sentence) > max_length:\n","            # find last punctuation before max_length\n","            split_pos = -1\n","            for p in punctuations:\n","                pos = sentence.rfind(p, 0, max_length + 1)\n","                if pos > split_pos:\n","                    split_pos = pos\n","            \n","            # If no punctuation found, split at the last space before max_length\n","            if split_pos == -1:\n","                split_pos = sentence.rfind(' ', 0, max_length + 1)\n","            \n","            # If no space found, just split at max_length\n","            if split_pos == -1:\n","                split_pos = max_length\n","            \n","            # Append the split segment to results\n","            results.append(sentence[:split_pos + 1].strip())\n","            # Move the rest of the sentence forward\n","            sentence = sentence[split_pos + 1:].strip()\n","        \n","        # Append the remainder of the sentence if it's not empty\n","        if sentence:\n","            results.append(sentence)\n","    \n","    return np.array(results)\n"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T14:43:10.270672Z","iopub.status.busy":"2024-05-04T14:43:10.270202Z","iopub.status.idle":"2024-05-04T14:43:14.879096Z","shell.execute_reply":"2024-05-04T14:43:14.877898Z","shell.execute_reply.started":"2024-05-04T14:43:10.270610Z"},"trusted":true},"outputs":[],"source":["# Raw sentences\n","raw_sentences = nlp_train[\"Sentence\"].values\n","# Split sentences\n","sentences = split_sentences(raw_sentences, max_length=200)\n","# Remove punctuations\n","sentences = [remove_punctuations(s).lower() for s in sentences]\n","# Apply convert to ascii to y_train\n","asci_sentences = [convert_to_ascii(s) for s in sentences]"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T14:43:14.881596Z","iopub.status.busy":"2024-05-04T14:43:14.881140Z","iopub.status.idle":"2024-05-04T14:43:14.888363Z","shell.execute_reply":"2024-05-04T14:43:14.886985Z","shell.execute_reply.started":"2024-05-04T14:43:14.881532Z"},"trusted":true},"outputs":[],"source":["def encode_word(sentence):\n","    for char in sentence:\n","        # print(char)\n","        if char in \"ıöüğşç\":\n","            # replace char with 2\n","            sentence = sentence.replace(char, '2')\n","        else:\n","            sentence = sentence.replace(char, '1')\n","    return sentence"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T14:43:14.890520Z","iopub.status.busy":"2024-05-04T14:43:14.890016Z","iopub.status.idle":"2024-05-04T14:43:21.838512Z","shell.execute_reply":"2024-05-04T14:43:21.837567Z","shell.execute_reply.started":"2024-05-04T14:43:14.890458Z"},"trusted":true},"outputs":[],"source":["sentences_diacritics = sentences.copy()\n","for sentence_index in range(len(sentences_diacritics)):\n","    sentence = sentences_diacritics[sentence_index]\n","    new_sentence = encode_word(sentence)\n","    sentences_diacritics[sentence_index] = new_sentence"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T14:43:21.840421Z","iopub.status.busy":"2024-05-04T14:43:21.839981Z","iopub.status.idle":"2024-05-04T14:43:21.848145Z","shell.execute_reply":"2024-05-04T14:43:21.847150Z","shell.execute_reply.started":"2024-05-04T14:43:21.840362Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'121211111111111122111111112112211111211111211121111111121111111211212111111111111111112111112111'"]},"execution_count":38,"metadata":{},"output_type":"execute_result"}],"source":["sentences_diacritics[0]"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T14:43:21.850128Z","iopub.status.busy":"2024-05-04T14:43:21.849675Z","iopub.status.idle":"2024-05-04T14:43:21.861454Z","shell.execute_reply":"2024-05-04T14:43:21.860583Z","shell.execute_reply.started":"2024-05-04T14:43:21.850072Z"},"trusted":true},"outputs":[],"source":["def align_tokens_with_diacritics(texts, diacritics, tokenizer, label_dict):\n","    tokenized_texts = tokenizer(texts, padding=True, truncation=True, return_offsets_mapping=True, return_tensors=\"pt\")\n","    input_ids = tokenized_texts['input_ids']\n","    attention_masks = tokenized_texts['attention_mask']\n","    token_type_ids = tokenized_texts['token_type_ids']\n","    offset_mappings = tokenized_texts['offset_mapping']\n","    \n","    # Initialize the labels for each token to a default numeric value, e.g., label_dict['O'] for 'Other'\n","    labels = [[label_dict['O']] * len(input_id) for input_id in input_ids]\n","\n","    for i, offsets in enumerate(offset_mappings):\n","        for token_index, (start, end) in enumerate(offsets):\n","            if start == end:  # Special tokens, skip them\n","                continue\n","            # Extract the original character sequence corresponding to the current token\n","            char_sequence = texts[i][start:end]\n","            # Assuming diacritics are stored in a similar structure as texts\n","            corresponding_diacritic = diacritics[i][start:end]\n","            # Assign the diacritic label to the first character's diacritic or any suitable logic\n","            # Use the dictionary to convert string labels to integers\n","            labels[i][token_index] = label_dict[corresponding_diacritic[0]] if corresponding_diacritic else label_dict['O']\n","\n","    return input_ids, attention_masks, token_type_ids, labels\n"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T14:43:21.863515Z","iopub.status.busy":"2024-05-04T14:43:21.862962Z","iopub.status.idle":"2024-05-04T14:44:29.117487Z","shell.execute_reply":"2024-05-04T14:44:29.116496Z","shell.execute_reply.started":"2024-05-04T14:43:21.863454Z"},"trusted":true},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-turkish-cased\")\n","label_dict = {'O': 0, '1': 1, '2': 2}\n","input_ids, attention_masks, token_type_ids, labels = align_tokens_with_diacritics(\n","    asci_sentences, sentences_diacritics, tokenizer, label_dict\n",")"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T14:44:29.119346Z","iopub.status.busy":"2024-05-04T14:44:29.118965Z","iopub.status.idle":"2024-05-04T14:44:29.329208Z","shell.execute_reply":"2024-05-04T14:44:29.328168Z","shell.execute_reply.started":"2024-05-04T14:44:29.119290Z"},"trusted":true},"outputs":[],"source":["# Convert lists of labels into a tensor; assuming each label is already converted to a numeric form\n","label_tensors = torch.tensor(labels, dtype=torch.long)\n","\n","# Create a TensorDataset\n","dataset = TensorDataset(input_ids, attention_masks, token_type_ids, label_tensors)\n","\n","# You can now use this dataset with a DataLoader to create batches\n","from torch.utils.data import DataLoader\n","dataloader = DataLoader(dataset, batch_size=16, shuffle=True)"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T14:44:29.330806Z","iopub.status.busy":"2024-05-04T14:44:29.330504Z","iopub.status.idle":"2024-05-04T14:44:34.428839Z","shell.execute_reply":"2024-05-04T14:44:34.427737Z","shell.execute_reply.started":"2024-05-04T14:44:29.330767Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-base-turkish-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","/Users/mustafa/miniconda3/envs/nlp_project/lib/python3.11/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["from transformers import BertForTokenClassification, AdamW\n","\n","# Assuming you have a specific number of diacritic labels\n","num_labels = len(set([label for sublist in labels for label in sublist]))  # Adjust this as necessary\n","\n","model = BertForTokenClassification.from_pretrained(\n","    \"dbmdz/bert-base-turkish-cased\",\n","    num_labels=num_labels,\n","    output_attentions=False,\n","    output_hidden_states=False,\n",")\n","\n","# Move model to GPU if available\n","model.to(device)\n","\n","# Setting up the optimizer\n","optimizer = AdamW(model.parameters(), lr=2e-5)\n"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2024-05-04T14:44:34.430912Z","iopub.status.busy":"2024-05-04T14:44:34.430460Z","iopub.status.idle":"2024-05-04T14:54:23.477411Z","shell.execute_reply":"2024-05-04T14:54:23.476499Z","shell.execute_reply.started":"2024-05-04T14:44:34.430847Z"},"trusted":true},"outputs":[],"source":["from tqdm import tqdm\n","\n","epochs = 1\n","\n","# model.train()\n","# for epoch in range(epochs):\n","#     total_loss = 0\n","#     for step, batch in enumerate(tqdm(dataloader)):\n","#         batch = tuple(t.to(device) for t in batch)\n","#         b_input_ids, b_input_mask, b_token_types, b_labels = batch\n","#         model.zero_grad()\n","        \n","#         outputs = model(b_input_ids, token_type_ids=b_token_types, attention_mask=b_input_mask, labels=b_labels)\n","#         loss = outputs[0]\n","#         total_loss += loss.item()\n","#         loss.backward()\n","#         optimizer.step()\n","\n","#     print(f\"Epoch {epoch}, Loss: {total_loss / len(dataloader)}\")"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/plain":["BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",")"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["model = torch.load('full_model.pth', map_location=torch.device('cpu'))\n","model.eval()"]},{"cell_type":"code","execution_count":47,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 0]])\n","Original: sınif havuz ve acik deniz calismalariyla tum dunyada\n","Diacritized: sınif havuz ve acik deniz calismalariyla tum dunyada\n"]}],"source":["# Input text\n","input_text = \"sinif havuz ve acik deniz calismalariyla tum dunyada\"\n","\n","# Tokenize the input\n","test_inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n","test_inputs = {k: v.to(device) for k, v in test_inputs.items()}\n","\n","# Predict\n","model.eval()  # Set the model to evaluation mode\n","with torch.no_grad():\n","    outputs = model(**test_inputs)\n","    predictions = torch.argmax(outputs.logits, dim=-1)\n","\n","# Get tokens for reference\n","tokens = tokenizer.convert_ids_to_tokens(test_inputs['input_ids'][0])\n","\n","# Define diacritics mapping\n","diacritics_mapping = {\n","    'c': {2: 'ç'},\n","    'g': {2: 'ğ'},\n","    'i': {2: 'ı'},\n","    'o': {2: 'ö'},\n","    's': {2: 'ş'},\n","    'u': {2: 'ü'}\n","}\n","\n","# Function to apply diacritics based on predictions\n","def apply_diacritics(tokens, predictions, mapping):\n","    diacritized_text = []\n","    skip_tokens = ['[CLS]', '[SEP]', '[PAD]']  # Tokens to skip when applying diacritics\n","\n","    # Accumulator for reassembling subword tokens\n","    current_word = \"\"\n","    for token, pred in zip(tokens, predictions[0]):\n","        if token in skip_tokens:\n","            continue\n","\n","        # Check if the token is a continuation of the previous one\n","        if token.startswith(\"##\"):\n","            current_word += token[2:]  # Append without \"##\" and without adding space\n","        else:\n","            # If there's accumulated word data from previous tokens, append it first\n","            if current_word:\n","                diacritized_text.append(current_word)\n","                current_word = \"\"\n","            # Start a new word accumulation\n","            current_word = token\n","\n","        # Apply diacritics to the current part of the word if applicable\n","        if current_word in mapping and pred.item() in mapping[current_word]:\n","            # Replace the whole word with its diacritized version\n","            current_word = mapping[current_word][pred.item()]\n","\n","    # Append the last accumulated word if any\n","    if current_word:\n","        diacritized_text.append(current_word)\n","\n","    return \" \".join(diacritized_text)\n","\n","# Apply diacritics\n","diacritized_output = apply_diacritics(tokens, predictions, diacritics_mapping)\n","print(predictions)\n","print(\"Original:\", input_text)\n","print(\"Diacritized:\", diacritized_output)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":843370,"sourceId":1439227,"sourceType":"datasetVersion"},{"datasetId":4789828,"sourceId":8108889,"sourceType":"datasetVersion"},{"datasetId":4789802,"sourceId":8283698,"sourceType":"datasetVersion"}],"dockerImageVersionId":29860,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":4}
